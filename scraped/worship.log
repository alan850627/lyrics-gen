Iteration 10000, batch loss: 0.012859, training accuracy: 0.24000
Iteration 20000, batch loss: 0.012962, training accuracy: 0.16000
Iteration 30000, batch loss: 0.012780, training accuracy: 0.22000
Iteration 40000, batch loss: 0.012387, training accuracy: 0.30000
Iteration 50000, batch loss: 0.012836, training accuracy: 0.12000
Iteration 60000, batch loss: 0.012044, training accuracy: 0.26000
Iteration 70000, batch loss: 0.011435, training accuracy: 0.36000
Iteration 80000, batch loss: 0.011135, training accuracy: 0.28000
Iteration 90000, batch loss: 0.010702, training accuracy: 0.46000
Iteration 100000, batch loss: 0.011152, training accuracy: 0.36000
Iteration 110000, batch loss: 0.010006, training accuracy: 0.36000
Iteration 120000, batch loss: 0.011131, training accuracy: 0.36000
Iteration 130000, batch loss: 0.012019, training accuracy: 0.26000
Iteration 140000, batch loss: 0.010154, training accuracy: 0.38000
Iteration 150000, batch loss: 0.009695, training accuracy: 0.48000
Iteration 160000, batch loss: 0.008914, training accuracy: 0.54000
Iteration 170000, batch loss: 0.010535, training accuracy: 0.40000
Iteration 180000, batch loss: 0.010458, training accuracy: 0.40000
Iteration 190000, batch loss: 0.009308, training accuracy: 0.50000
Iteration 200000, batch loss: 0.009712, training accuracy: 0.46000
Iteration 210000, batch loss: 0.010258, training accuracy: 0.30000
Iteration 220000, batch loss: 0.010509, training accuracy: 0.36000
Iteration 230000, batch loss: 0.004187, training accuracy: 0.82000
Iteration 240000, batch loss: 0.008690, training accuracy: 0.52000
Iteration 250000, batch loss: 0.009264, training accuracy: 0.48000
Iteration 260000, batch loss: 0.009526, training accuracy: 0.50000
Iteration 270000, batch loss: 0.009295, training accuracy: 0.52000
Iteration 280000, batch loss: 0.010606, training accuracy: 0.38000
Iteration 290000, batch loss: 0.009152, training accuracy: 0.40000
Iteration 300000, batch loss: 0.008026, training accuracy: 0.58000
Iteration 310000, batch loss: 0.007675, training accuracy: 0.60000
Iteration 320000, batch loss: 0.007762, training accuracy: 0.60000
Iteration 330000, batch loss: 0.008138, training accuracy: 0.60000
Iteration 340000, batch loss: 0.008625, training accuracy: 0.56000
Iteration 350000, batch loss: 0.008995, training accuracy: 0.50000
Iteration 360000, batch loss: 0.009271, training accuracy: 0.50000
Iteration 370000, batch loss: 0.008515, training accuracy: 0.52000
Iteration 380000, batch loss: 0.007766, training accuracy: 0.66000
Iteration 390000, batch loss: 0.006638, training accuracy: 0.66000
Iteration 400000, batch loss: 0.007261, training accuracy: 0.66000
Iteration 410000, batch loss: 0.008511, training accuracy: 0.56000
Iteration 420000, batch loss: 0.005734, training accuracy: 0.80000
Iteration 430000, batch loss: 0.007048, training accuracy: 0.74000
Iteration 440000, batch loss: 0.008192, training accuracy: 0.60000
Iteration 450000, batch loss: 0.007941, training accuracy: 0.64000
Iteration 460000, batch loss: 0.003187, training accuracy: 0.86000
Iteration 470000, batch loss: 0.006555, training accuracy: 0.66000
Iteration 480000, batch loss: 0.006134, training accuracy: 0.64000
Iteration 490000, batch loss: 0.007086, training accuracy: 0.66000
Iteration 500000, batch loss: 0.006297, training accuracy: 0.72000
Iteration 510000, batch loss: 0.007495, training accuracy: 0.64000
Iteration 520000, batch loss: 0.006464, training accuracy: 0.74000
Iteration 530000, batch loss: 0.005163, training accuracy: 0.80000
Iteration 540000, batch loss: 0.004679, training accuracy: 0.80000
Iteration 550000, batch loss: 0.004087, training accuracy: 0.86000
Iteration 560000, batch loss: 0.004562, training accuracy: 0.86000
Iteration 570000, batch loss: 0.006121, training accuracy: 0.74000
Iteration 580000, batch loss: 0.006002, training accuracy: 0.72000
Iteration 590000, batch loss: 0.025542, training accuracy: 0.08000
Iteration 600000, batch loss: 0.015845, training accuracy: 0.32000
Iteration 610000, batch loss: 0.013904, training accuracy: 0.34000
Iteration 620000, batch loss: 0.012948, training accuracy: 0.36000
Iteration 630000, batch loss: 0.013293, training accuracy: 0.30000
Iteration 640000, batch loss: 0.013165, training accuracy: 0.38000
Iteration 650000, batch loss: 0.012365, training accuracy: 0.38000
Iteration 660000, batch loss: 0.012627, training accuracy: 0.32000
Iteration 670000, batch loss: 0.012779, training accuracy: 0.28000
Iteration 680000, batch loss: 0.012739, training accuracy: 0.30000
Iteration 690000, batch loss: 0.012981, training accuracy: 0.10000
Iteration 700000, batch loss: 0.011318, training accuracy: 0.40000
Iteration 710000, batch loss: 0.011834, training accuracy: 0.32000
Iteration 720000, batch loss: 0.012208, training accuracy: 0.36000
Iteration 730000, batch loss: 0.011502, training accuracy: 0.36000
Iteration 740000, batch loss: 0.012342, training accuracy: 0.20000
Iteration 750000, batch loss: 0.011586, training accuracy: 0.32000
Iteration 760000, batch loss: 0.010879, training accuracy: 0.42000
Iteration 770000, batch loss: 0.010792, training accuracy: 0.38000
Iteration 780000, batch loss: 0.010264, training accuracy: 0.46000
Iteration 790000, batch loss: 0.010671, training accuracy: 0.40000
Iteration 800000, batch loss: 0.010692, training accuracy: 0.38000
Iteration 810000, batch loss: 0.010835, training accuracy: 0.34000
Iteration 820000, batch loss: 0.012368, training accuracy: 0.20000
Iteration 830000, batch loss: 0.010369, training accuracy: 0.42000
Iteration 840000, batch loss: 0.010009, training accuracy: 0.44000
Iteration 850000, batch loss: 0.008501, training accuracy: 0.54000
Iteration 860000, batch loss: 0.009863, training accuracy: 0.42000
Iteration 870000, batch loss: 0.010542, training accuracy: 0.42000
Iteration 880000, batch loss: 0.008917, training accuracy: 0.48000
Iteration 890000, batch loss: 0.010359, training accuracy: 0.50000
Iteration 900000, batch loss: 0.010090, training accuracy: 0.34000
Iteration 910000, batch loss: 0.010153, training accuracy: 0.44000
Iteration 920000, batch loss: 0.006514, training accuracy: 0.76000
Iteration 930000, batch loss: 0.008660, training accuracy: 0.50000
Iteration 940000, batch loss: 0.009605, training accuracy: 0.42000
Iteration 950000, batch loss: 0.009496, training accuracy: 0.46000
Iteration 960000, batch loss: 0.008801, training accuracy: 0.58000
Iteration 970000, batch loss: 0.010116, training accuracy: 0.38000
Iteration 980000, batch loss: 0.008590, training accuracy: 0.54000
Iteration 990000, batch loss: 0.008017, training accuracy: 0.62000
Iteration 1000000, batch loss: 0.007638, training accuracy: 0.64000
Iteration 1010000, batch loss: 0.007543, training accuracy: 0.56000
Iteration 1020000, batch loss: 0.007968, training accuracy: 0.62000
Iteration 1030000, batch loss: 0.008669, training accuracy: 0.56000
Iteration 1040000, batch loss: 0.008766, training accuracy: 0.52000
Iteration 1050000, batch loss: 0.009353, training accuracy: 0.44000
Iteration 1060000, batch loss: 0.007619, training accuracy: 0.56000
Iteration 1070000, batch loss: 0.007008, training accuracy: 0.64000
Iteration 1080000, batch loss: 0.005754, training accuracy: 0.70000
Iteration 1090000, batch loss: 0.006857, training accuracy: 0.60000
Iteration 1100000, batch loss: 0.007553, training accuracy: 0.60000
Iteration 1110000, batch loss: 0.005037, training accuracy: 0.82000
Iteration 1120000, batch loss: 0.006381, training accuracy: 0.72000
Iteration 1130000, batch loss: 0.006700, training accuracy: 0.68000
Iteration 1140000, batch loss: 0.006829, training accuracy: 0.64000
Iteration 1150000, batch loss: 0.002931, training accuracy: 0.88000
Iteration 1160000, batch loss: 0.006091, training accuracy: 0.76000
Iteration 1170000, batch loss: 0.005649, training accuracy: 0.74000
Iteration 1180000, batch loss: 0.005728, training accuracy: 0.70000
Iteration 1190000, batch loss: 0.005061, training accuracy: 0.78000
Iteration 1200000, batch loss: 0.006961, training accuracy: 0.70000
Iteration 1210000, batch loss: 0.005311, training accuracy: 0.82000
Iteration 1220000, batch loss: 0.004546, training accuracy: 0.80000
Iteration 1230000, batch loss: 0.004106, training accuracy: 0.82000
Iteration 1240000, batch loss: 0.004069, training accuracy: 0.84000
Iteration 1250000, batch loss: 0.004577, training accuracy: 0.86000
Iteration 1260000, batch loss: 0.006105, training accuracy: 0.72000
Iteration 1270000, batch loss: 0.005063, training accuracy: 0.74000
Iteration 1280000, batch loss: 0.004725, training accuracy: 0.78000
Iteration 1290000, batch loss: 0.004592, training accuracy: 0.82000
Iteration 1300000, batch loss: 0.004491, training accuracy: 0.84000
Iteration 1310000, batch loss: 0.003339, training accuracy: 0.86000
Iteration 1320000, batch loss: 0.005281, training accuracy: 0.76000
Iteration 1330000, batch loss: 0.006097, training accuracy: 0.74000
Iteration 1340000, batch loss: 0.003171, training accuracy: 0.88000
Iteration 1350000, batch loss: 0.004379, training accuracy: 0.80000
Iteration 1360000, batch loss: 0.003951, training accuracy: 0.86000
Iteration 1370000, batch loss: 0.004890, training accuracy: 0.76000
Iteration 1380000, batch loss: 0.003322, training accuracy: 0.96000
