Iteration 10000, batch loss: 0.011802, training accuracy: 0.16000
Iteration 20000, batch loss: 0.011174, training accuracy: 0.20000
Iteration 30000, batch loss: 0.011100, training accuracy: 0.16000
Iteration 40000, batch loss: 0.011147, training accuracy: 0.18000
Iteration 50000, batch loss: 0.011042, training accuracy: 0.18000
Iteration 60000, batch loss: 0.010560, training accuracy: 0.28000
Iteration 70000, batch loss: 0.010793, training accuracy: 0.28000
Iteration 80000, batch loss: 0.010783, training accuracy: 0.26000
Iteration 90000, batch loss: 0.010430, training accuracy: 0.30000
Iteration 100000, batch loss: 0.010371, training accuracy: 0.32000
Iteration 110000, batch loss: 0.009556, training accuracy: 0.36000
Iteration 120000, batch loss: 0.010438, training accuracy: 0.40000
Iteration 130000, batch loss: 0.009866, training accuracy: 0.36000
Iteration 140000, batch loss: 0.010356, training accuracy: 0.26000
Iteration 150000, batch loss: 0.009666, training accuracy: 0.30000
Iteration 160000, batch loss: 0.008835, training accuracy: 0.46000
Iteration 170000, batch loss: 0.009389, training accuracy: 0.34000
Iteration 180000, batch loss: 0.010374, training accuracy: 0.22000
Iteration 190000, batch loss: 0.009479, training accuracy: 0.38000
Iteration 200000, batch loss: 0.008766, training accuracy: 0.46000
Iteration 210000, batch loss: 0.009230, training accuracy: 0.42000
Iteration 220000, batch loss: 0.009275, training accuracy: 0.36000
Iteration 230000, batch loss: 0.008728, training accuracy: 0.42000
Iteration 240000, batch loss: 0.008834, training accuracy: 0.44000
Iteration 250000, batch loss: 0.010051, training accuracy: 0.40000
Iteration 260000, batch loss: 0.007937, training accuracy: 0.50000
Iteration 270000, batch loss: 0.009065, training accuracy: 0.40000
Iteration 280000, batch loss: 0.010564, training accuracy: 0.26000
Iteration 290000, batch loss: 0.008409, training accuracy: 0.48000
Iteration 300000, batch loss: 0.009464, training accuracy: 0.36000
Iteration 310000, batch loss: 0.009392, training accuracy: 0.38000
Iteration 320000, batch loss: 0.009334, training accuracy: 0.40000
Iteration 330000, batch loss: 0.009679, training accuracy: 0.32000
Iteration 340000, batch loss: 0.009481, training accuracy: 0.32000
Iteration 350000, batch loss: 0.008813, training accuracy: 0.38000
Iteration 360000, batch loss: 0.008280, training accuracy: 0.48000
Iteration 370000, batch loss: 0.008531, training accuracy: 0.46000
Iteration 380000, batch loss: 0.007622, training accuracy: 0.56000
Iteration 390000, batch loss: 0.009279, training accuracy: 0.36000
Iteration 400000, batch loss: 0.009083, training accuracy: 0.38000
Iteration 410000, batch loss: 0.009133, training accuracy: 0.40000
Iteration 420000, batch loss: 0.006507, training accuracy: 0.62000
Iteration 430000, batch loss: 0.008206, training accuracy: 0.48000
Iteration 440000, batch loss: 0.008710, training accuracy: 0.52000
Iteration 450000, batch loss: 0.009552, training accuracy: 0.40000
Iteration 460000, batch loss: 0.008286, training accuracy: 0.48000
Iteration 470000, batch loss: 0.007432, training accuracy: 0.58000
Iteration 480000, batch loss: 0.007728, training accuracy: 0.50000
Iteration 490000, batch loss: 0.007133, training accuracy: 0.62000
Iteration 500000, batch loss: 0.006144, training accuracy: 0.62000
Iteration 510000, batch loss: 0.008384, training accuracy: 0.44000
Iteration 520000, batch loss: 0.007519, training accuracy: 0.56000
Iteration 530000, batch loss: 0.006947, training accuracy: 0.58000
Iteration 540000, batch loss: 0.006905, training accuracy: 0.58000
Iteration 550000, batch loss: 0.005754, training accuracy: 0.68000
Iteration 560000, batch loss: 0.006982, training accuracy: 0.52000
Iteration 570000, batch loss: 0.006369, training accuracy: 0.60000
Iteration 580000, batch loss: 0.006496, training accuracy: 0.62000
Iteration 590000, batch loss: 0.005909, training accuracy: 0.74000
Iteration 600000, batch loss: 0.005101, training accuracy: 0.74000
Iteration 610000, batch loss: 0.007017, training accuracy: 0.54000
Iteration 620000, batch loss: 0.005757, training accuracy: 0.72000
Iteration 630000, batch loss: 0.004686, training accuracy: 0.70000
Iteration 640000, batch loss: 0.005358, training accuracy: 0.68000
Iteration 650000, batch loss: 0.005003, training accuracy: 0.74000
Iteration 660000, batch loss: 0.004167, training accuracy: 0.80000
Iteration 670000, batch loss: 0.005369, training accuracy: 0.70000
Iteration 680000, batch loss: 0.005506, training accuracy: 0.74000
Iteration 690000, batch loss: 0.004380, training accuracy: 0.78000
Iteration 700000, batch loss: 0.004802, training accuracy: 0.76000
Iteration 710000, batch loss: 0.005033, training accuracy: 0.70000
Iteration 720000, batch loss: 0.005245, training accuracy: 0.68000
Iteration 730000, batch loss: 0.004064, training accuracy: 0.86000
Iteration 740000, batch loss: 0.004493, training accuracy: 0.78000
Iteration 750000, batch loss: 0.004482, training accuracy: 0.72000
Iteration 760000, batch loss: 0.004321, training accuracy: 0.72000
Iteration 770000, batch loss: 0.003799, training accuracy: 0.84000
Iteration 780000, batch loss: 0.004964, training accuracy: 0.74000
Iteration 790000, batch loss: 0.004283, training accuracy: 0.78000
Iteration 800000, batch loss: 0.004997, training accuracy: 0.72000
Iteration 810000, batch loss: 0.003586, training accuracy: 0.84000
Iteration 820000, batch loss: 0.004087, training accuracy: 0.80000
Iteration 830000, batch loss: 0.004514, training accuracy: 0.72000
Iteration 840000, batch loss: 0.004192, training accuracy: 0.76000
Iteration 850000, batch loss: 0.003473, training accuracy: 0.84000
Iteration 860000, batch loss: 0.003672, training accuracy: 0.86000
Iteration 870000, batch loss: 0.002728, training accuracy: 0.86000
Iteration 880000, batch loss: 0.003312, training accuracy: 0.84000
Iteration 890000, batch loss: 0.003135, training accuracy: 0.86000
Iteration 900000, batch loss: 0.002155, training accuracy: 0.90000
Iteration 910000, batch loss: 0.003648, training accuracy: 0.76000
Iteration 920000, batch loss: 0.002009, training accuracy: 0.94000
Iteration 930000, batch loss: 0.002446, training accuracy: 0.88000
Iteration 940000, batch loss: 0.004497, training accuracy: 0.72000
Iteration 950000, batch loss: 0.003308, training accuracy: 0.84000
Iteration 960000, batch loss: 0.002878, training accuracy: 0.90000
Iteration 970000, batch loss: 0.003487, training accuracy: 0.82000
Iteration 980000, batch loss: 0.003040, training accuracy: 0.82000
Iteration 990000, batch loss: 0.002959, training accuracy: 0.88000
Iteration 1000000, batch loss: 0.004188, training accuracy: 0.76000
Iteration 1010000, batch loss: 0.003135, training accuracy: 0.86000
Iteration 1020000, batch loss: 0.002239, training accuracy: 0.90000
Iteration 1030000, batch loss: 0.003093, training accuracy: 0.82000
Iteration 1040000, batch loss: 0.002541, training accuracy: 0.86000
Iteration 1050000, batch loss: 0.003630, training accuracy: 0.76000
Iteration 1060000, batch loss: 0.002948, training accuracy: 0.82000
Iteration 1070000, batch loss: 0.002710, training accuracy: 0.86000
Iteration 1080000, batch loss: 0.001790, training accuracy: 0.96000
Iteration 1090000, batch loss: 0.002606, training accuracy: 0.90000
Iteration 1100000, batch loss: 0.002033, training accuracy: 0.92000
Iteration 1110000, batch loss: 0.002200, training accuracy: 0.92000
Iteration 1120000, batch loss: 0.001932, training accuracy: 0.90000
Iteration 1130000, batch loss: 0.003266, training accuracy: 0.82000
Iteration 1140000, batch loss: 0.002857, training accuracy: 0.84000
Iteration 1150000, batch loss: 0.002474, training accuracy: 0.82000
Iteration 1160000, batch loss: 0.001564, training accuracy: 0.94000
Iteration 1170000, batch loss: 0.002360, training accuracy: 0.86000
Iteration 1180000, batch loss: 0.002268, training accuracy: 0.90000
Iteration 1190000, batch loss: 0.001837, training accuracy: 0.90000
Iteration 1200000, batch loss: 0.001797, training accuracy: 0.90000
Iteration 1210000, batch loss: 0.002817, training accuracy: 0.86000
Iteration 1220000, batch loss: 0.002478, training accuracy: 0.84000
Iteration 1230000, batch loss: 0.002010, training accuracy: 0.88000
Iteration 1240000, batch loss: 0.001682, training accuracy: 0.94000
Iteration 1250000, batch loss: 0.001881, training accuracy: 0.92000
Iteration 1260000, batch loss: 0.001163, training accuracy: 0.96000
Iteration 1270000, batch loss: 0.001573, training accuracy: 0.94000
Iteration 1280000, batch loss: 0.001956, training accuracy: 0.90000
Iteration 1290000, batch loss: 0.001876, training accuracy: 0.92000
Iteration 1300000, batch loss: 0.001434, training accuracy: 0.94000
Iteration 1310000, batch loss: 0.001932, training accuracy: 0.88000
Iteration 1320000, batch loss: 0.001840, training accuracy: 0.94000
Iteration 1330000, batch loss: 0.001868, training accuracy: 0.92000
Iteration 1340000, batch loss: 0.001338, training accuracy: 0.94000
Iteration 1350000, batch loss: 0.001952, training accuracy: 0.92000
Iteration 1360000, batch loss: 0.001348, training accuracy: 0.94000
Iteration 1370000, batch loss: 0.001881, training accuracy: 0.90000
Iteration 1380000, batch loss: 0.001715, training accuracy: 0.88000
Iteration 1390000, batch loss: 0.001165, training accuracy: 0.94000
Iteration 1400000, batch loss: 0.001544, training accuracy: 0.92000
Iteration 1410000, batch loss: 0.001354, training accuracy: 0.96000
Iteration 1420000, batch loss: 0.001964, training accuracy: 0.90000
Iteration 1430000, batch loss: 0.001124, training accuracy: 0.98000
Iteration 1440000, batch loss: 0.001130, training accuracy: 0.96000
Iteration 1450000, batch loss: 0.001106, training accuracy: 0.94000
Iteration 1460000, batch loss: 0.001696, training accuracy: 0.92000
Iteration 1470000, batch loss: 0.001893, training accuracy: 0.96000
Iteration 1480000, batch loss: 0.001461, training accuracy: 0.98000
Iteration 1490000, batch loss: 0.002375, training accuracy: 0.86000
Iteration 1500000, batch loss: 0.001268, training accuracy: 0.96000
Iteration 1510000, batch loss: 0.001355, training accuracy: 0.94000
Iteration 1520000, batch loss: 0.000739, training accuracy: 0.98000
Iteration 1530000, batch loss: 0.001478, training accuracy: 0.94000
Iteration 1540000, batch loss: 0.001204, training accuracy: 0.96000
Iteration 1550000, batch loss: 0.000926, training accuracy: 0.96000
Iteration 1560000, batch loss: 0.001383, training accuracy: 0.96000
Iteration 1570000, batch loss: 0.001841, training accuracy: 0.96000
Iteration 1580000, batch loss: 0.001554, training accuracy: 1.00000
Iteration 1590000, batch loss: 0.001269, training accuracy: 0.96000
Iteration 1600000, batch loss: 0.001199, training accuracy: 0.94000
Iteration 1610000, batch loss: 0.001521, training accuracy: 0.92000
Iteration 1620000, batch loss: 0.000520, training accuracy: 1.00000
Iteration 1630000, batch loss: 0.001207, training accuracy: 0.96000
Iteration 1640000, batch loss: 0.001294, training accuracy: 0.94000
Iteration 1650000, batch loss: 0.000976, training accuracy: 0.98000
Iteration 1660000, batch loss: 0.001523, training accuracy: 0.92000
