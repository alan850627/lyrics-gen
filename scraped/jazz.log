Iteration 10000, batch loss: 0.012155, training accuracy: 0.24000
Iteration 20000, batch loss: 0.012008, training accuracy: 0.16000
Iteration 30000, batch loss: 0.011302, training accuracy: 0.22000
Iteration 40000, batch loss: 0.011676, training accuracy: 0.26000
Iteration 50000, batch loss: 0.010664, training accuracy: 0.32000
Iteration 60000, batch loss: 0.010836, training accuracy: 0.28000
Iteration 70000, batch loss: 0.010096, training accuracy: 0.46000
Iteration 80000, batch loss: 0.010570, training accuracy: 0.30000
Iteration 90000, batch loss: 0.011550, training accuracy: 0.20000
Iteration 100000, batch loss: 0.010078, training accuracy: 0.38000
Iteration 110000, batch loss: 0.009833, training accuracy: 0.36000
Iteration 120000, batch loss: 0.010847, training accuracy: 0.34000
Iteration 130000, batch loss: 0.010363, training accuracy: 0.28000
Iteration 140000, batch loss: 0.009948, training accuracy: 0.40000
Iteration 150000, batch loss: 0.010804, training accuracy: 0.32000
Iteration 160000, batch loss: 0.010643, training accuracy: 0.34000
Iteration 170000, batch loss: 0.010909, training accuracy: 0.26000
Iteration 180000, batch loss: 0.010516, training accuracy: 0.34000
Iteration 190000, batch loss: 0.008997, training accuracy: 0.46000
Iteration 200000, batch loss: 0.010036, training accuracy: 0.32000
Iteration 210000, batch loss: 0.009193, training accuracy: 0.48000
Iteration 220000, batch loss: 0.010012, training accuracy: 0.34000
Iteration 230000, batch loss: 0.009207, training accuracy: 0.40000
Iteration 240000, batch loss: 0.008281, training accuracy: 0.52000
Iteration 250000, batch loss: 0.008489, training accuracy: 0.52000
Iteration 260000, batch loss: 0.010013, training accuracy: 0.36000
Iteration 270000, batch loss: 0.009237, training accuracy: 0.42000
Iteration 280000, batch loss: 0.008229, training accuracy: 0.54000
Iteration 290000, batch loss: 0.008878, training accuracy: 0.52000
Iteration 300000, batch loss: 0.008118, training accuracy: 0.56000
Iteration 310000, batch loss: 0.008011, training accuracy: 0.58000
Iteration 320000, batch loss: 0.009542, training accuracy: 0.42000
Iteration 330000, batch loss: 0.009356, training accuracy: 0.44000
Iteration 340000, batch loss: 0.007628, training accuracy: 0.56000
Iteration 350000, batch loss: 0.008686, training accuracy: 0.50000
Iteration 360000, batch loss: 0.009501, training accuracy: 0.48000
Iteration 370000, batch loss: 0.008759, training accuracy: 0.46000
Iteration 380000, batch loss: 0.009069, training accuracy: 0.50000
Iteration 390000, batch loss: 0.007911, training accuracy: 0.50000
Iteration 400000, batch loss: 0.008511, training accuracy: 0.54000
Iteration 410000, batch loss: 0.008904, training accuracy: 0.40000
Iteration 420000, batch loss: 0.008802, training accuracy: 0.44000
Iteration 430000, batch loss: 0.008867, training accuracy: 0.42000
Iteration 440000, batch loss: 0.010560, training accuracy: 0.46000
Iteration 450000, batch loss: 0.010511, training accuracy: 0.40000
Iteration 460000, batch loss: 0.009526, training accuracy: 0.38000
Iteration 470000, batch loss: 0.010016, training accuracy: 0.38000
Iteration 480000, batch loss: 0.009794, training accuracy: 0.34000
Iteration 490000, batch loss: 0.009128, training accuracy: 0.38000
Iteration 500000, batch loss: 0.008531, training accuracy: 0.56000
Iteration 510000, batch loss: 0.008663, training accuracy: 0.46000
Iteration 520000, batch loss: 0.009375, training accuracy: 0.40000
Iteration 530000, batch loss: 0.007888, training accuracy: 0.58000
Iteration 540000, batch loss: 0.008806, training accuracy: 0.44000
Iteration 550000, batch loss: 0.007986, training accuracy: 0.46000
Iteration 560000, batch loss: 0.008585, training accuracy: 0.48000
Iteration 570000, batch loss: 0.009008, training accuracy: 0.38000
Iteration 580000, batch loss: 0.010283, training accuracy: 0.38000
Iteration 590000, batch loss: 0.009005, training accuracy: 0.42000
Iteration 600000, batch loss: 0.008052, training accuracy: 0.52000
Iteration 610000, batch loss: 0.007910, training accuracy: 0.56000
Iteration 620000, batch loss: 0.007739, training accuracy: 0.56000
Iteration 630000, batch loss: 0.009189, training accuracy: 0.44000
Iteration 640000, batch loss: 0.008497, training accuracy: 0.46000
Iteration 650000, batch loss: 0.009120, training accuracy: 0.44000
Iteration 660000, batch loss: 0.007680, training accuracy: 0.54000
Iteration 670000, batch loss: 0.007794, training accuracy: 0.52000
Iteration 680000, batch loss: 0.007484, training accuracy: 0.54000
Iteration 690000, batch loss: 0.008223, training accuracy: 0.56000
Iteration 700000, batch loss: 0.008543, training accuracy: 0.46000
Iteration 710000, batch loss: 0.008654, training accuracy: 0.56000
Iteration 720000, batch loss: 0.008563, training accuracy: 0.48000
Iteration 730000, batch loss: 0.008783, training accuracy: 0.44000
Iteration 740000, batch loss: 0.008987, training accuracy: 0.40000
Iteration 750000, batch loss: 0.007343, training accuracy: 0.56000
Iteration 760000, batch loss: 0.006933, training accuracy: 0.56000
Iteration 770000, batch loss: 0.007550, training accuracy: 0.54000
Iteration 780000, batch loss: 0.008862, training accuracy: 0.44000
Iteration 790000, batch loss: 0.007511, training accuracy: 0.54000
Iteration 800000, batch loss: 0.007650, training accuracy: 0.56000
Iteration 810000, batch loss: 0.008338, training accuracy: 0.58000
Iteration 820000, batch loss: 0.008357, training accuracy: 0.44000
Iteration 830000, batch loss: 0.007654, training accuracy: 0.58000
Iteration 840000, batch loss: 0.008190, training accuracy: 0.48000
Iteration 850000, batch loss: 0.008694, training accuracy: 0.46000
Iteration 860000, batch loss: 0.009149, training accuracy: 0.34000
Iteration 870000, batch loss: 0.007869, training accuracy: 0.52000
Iteration 880000, batch loss: 0.007064, training accuracy: 0.66000
Iteration 890000, batch loss: 0.007710, training accuracy: 0.54000
Iteration 900000, batch loss: 0.007238, training accuracy: 0.60000
Iteration 910000, batch loss: 0.008553, training accuracy: 0.48000
Iteration 920000, batch loss: 0.007793, training accuracy: 0.46000
Iteration 930000, batch loss: 0.006599, training accuracy: 0.66000
Iteration 940000, batch loss: 0.006647, training accuracy: 0.64000
Iteration 950000, batch loss: 0.007895, training accuracy: 0.48000
Iteration 960000, batch loss: 0.007824, training accuracy: 0.50000
Iteration 970000, batch loss: 0.006734, training accuracy: 0.62000
Iteration 980000, batch loss: 0.007445, training accuracy: 0.52000
Iteration 990000, batch loss: 0.006084, training accuracy: 0.66000
Iteration 1000000, batch loss: 0.006723, training accuracy: 0.62000
Iteration 1010000, batch loss: 0.007435, training accuracy: 0.64000
Iteration 1020000, batch loss: 0.008200, training accuracy: 0.50000
Iteration 1030000, batch loss: 0.006241, training accuracy: 0.66000
Iteration 1040000, batch loss: 0.007499, training accuracy: 0.58000
Iteration 1050000, batch loss: 0.007700, training accuracy: 0.60000
Iteration 1060000, batch loss: 0.006918, training accuracy: 0.56000
Iteration 1070000, batch loss: 0.007136, training accuracy: 0.58000
Iteration 1080000, batch loss: 0.006230, training accuracy: 0.68000
Iteration 1090000, batch loss: 0.006299, training accuracy: 0.66000
Iteration 1100000, batch loss: 0.007626, training accuracy: 0.56000
Iteration 1110000, batch loss: 0.007420, training accuracy: 0.54000
Iteration 1120000, batch loss: 0.008292, training accuracy: 0.52000
Iteration 1130000, batch loss: 0.005698, training accuracy: 0.72000
Iteration 1140000, batch loss: 0.007384, training accuracy: 0.56000
Iteration 1150000, batch loss: 0.006566, training accuracy: 0.60000
Iteration 1160000, batch loss: 0.006780, training accuracy: 0.60000
Iteration 1170000, batch loss: 0.006620, training accuracy: 0.62000
Iteration 1180000, batch loss: 0.006290, training accuracy: 0.70000
Iteration 1190000, batch loss: 0.005599, training accuracy: 0.74000
Iteration 1200000, batch loss: 0.005394, training accuracy: 0.68000
Iteration 1210000, batch loss: 0.006086, training accuracy: 0.62000
Iteration 1220000, batch loss: 0.005777, training accuracy: 0.66000
Iteration 1230000, batch loss: 0.006597, training accuracy: 0.62000
Iteration 1240000, batch loss: 0.005291, training accuracy: 0.68000
Iteration 1250000, batch loss: 0.006195, training accuracy: 0.64000
Iteration 1260000, batch loss: 0.005461, training accuracy: 0.70000
Iteration 1270000, batch loss: 0.007416, training accuracy: 0.58000
Iteration 1280000, batch loss: 0.005424, training accuracy: 0.74000
Iteration 1290000, batch loss: 0.004028, training accuracy: 0.86000
Iteration 1300000, batch loss: 0.005095, training accuracy: 0.72000
Iteration 1310000, batch loss: 0.005006, training accuracy: 0.72000
Iteration 1320000, batch loss: 0.005765, training accuracy: 0.70000
Iteration 1330000, batch loss: 0.004841, training accuracy: 0.78000
Iteration 1340000, batch loss: 0.005410, training accuracy: 0.70000
Iteration 1350000, batch loss: 0.004809, training accuracy: 0.76000
Iteration 1360000, batch loss: 0.004739, training accuracy: 0.80000
Iteration 1370000, batch loss: 0.003644, training accuracy: 0.82000
Iteration 1380000, batch loss: 0.004833, training accuracy: 0.74000
Iteration 1390000, batch loss: 0.005238, training accuracy: 0.70000
Iteration 1400000, batch loss: 0.004145, training accuracy: 0.82000
Iteration 1410000, batch loss: 0.005650, training accuracy: 0.70000
Iteration 1420000, batch loss: 0.004111, training accuracy: 0.84000
Iteration 1430000, batch loss: 0.005739, training accuracy: 0.62000
Iteration 1440000, batch loss: 0.003207, training accuracy: 0.86000
Iteration 1450000, batch loss: 0.003615, training accuracy: 0.82000
Iteration 1460000, batch loss: 0.003530, training accuracy: 0.82000
Iteration 1470000, batch loss: 0.004726, training accuracy: 0.72000
Iteration 1480000, batch loss: 0.004045, training accuracy: 0.76000
Iteration 1490000, batch loss: 0.003434, training accuracy: 0.80000
Iteration 1500000, batch loss: 0.003585, training accuracy: 0.84000
Iteration 1510000, batch loss: 0.004138, training accuracy: 0.76000
Iteration 1520000, batch loss: 0.003517, training accuracy: 0.80000
Iteration 1530000, batch loss: 0.004049, training accuracy: 0.72000
Iteration 1540000, batch loss: 0.004062, training accuracy: 0.80000
Iteration 1550000, batch loss: 0.004731, training accuracy: 0.70000
Iteration 1560000, batch loss: 0.003575, training accuracy: 0.78000
Iteration 1570000, batch loss: 0.002242, training accuracy: 0.90000
Iteration 1580000, batch loss: 0.002595, training accuracy: 0.90000
Iteration 1590000, batch loss: 0.002266, training accuracy: 0.90000
Iteration 1600000, batch loss: 0.003423, training accuracy: 0.82000
Iteration 1610000, batch loss: 0.003083, training accuracy: 0.80000
Iteration 1620000, batch loss: 0.002498, training accuracy: 0.90000
Iteration 1630000, batch loss: 0.002876, training accuracy: 0.86000
Iteration 1640000, batch loss: 0.003349, training accuracy: 0.84000
Iteration 1650000, batch loss: 0.002661, training accuracy: 0.86000
