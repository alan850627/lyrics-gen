Iteration 5000, batch loss: 0.013900, training accuracy: 0.12000
Iteration 10000, batch loss: 0.013377, training accuracy: 0.20000
Iteration 15000, batch loss: 0.013441, training accuracy: 0.16000
Iteration 20000, batch loss: 0.012264, training accuracy: 0.22000
Iteration 25000, batch loss: 0.012261, training accuracy: 0.30000
Iteration 30000, batch loss: 0.012680, training accuracy: 0.18000
Iteration 35000, batch loss: 0.012219, training accuracy: 0.34000
Iteration 40000, batch loss: 0.011816, training accuracy: 0.34000
Iteration 45000, batch loss: 0.011958, training accuracy: 0.32000
Iteration 50000, batch loss: 0.011327, training accuracy: 0.34000
Iteration 55000, batch loss: 0.011971, training accuracy: 0.26000
Iteration 60000, batch loss: 0.010976, training accuracy: 0.38000
Iteration 65000, batch loss: 0.011359, training accuracy: 0.44000
Iteration 70000, batch loss: 0.011302, training accuracy: 0.38000
Iteration 75000, batch loss: 0.011014, training accuracy: 0.38000
Iteration 80000, batch loss: 0.010754, training accuracy: 0.36000
Iteration 85000, batch loss: 0.010515, training accuracy: 0.40000
Iteration 90000, batch loss: 0.011445, training accuracy: 0.32000
Iteration 95000, batch loss: 0.010607, training accuracy: 0.40000
Iteration 100000, batch loss: 0.011305, training accuracy: 0.32000
Iteration 105000, batch loss: 0.009080, training accuracy: 0.52000
Iteration 110000, batch loss: 0.010359, training accuracy: 0.38000
Iteration 115000, batch loss: 0.010752, training accuracy: 0.46000
Iteration 120000, batch loss: 0.011114, training accuracy: 0.38000
Iteration 125000, batch loss: 0.010626, training accuracy: 0.38000
Iteration 130000, batch loss: 0.011309, training accuracy: 0.34000
Iteration 135000, batch loss: 0.009394, training accuracy: 0.52000
Iteration 140000, batch loss: 0.010621, training accuracy: 0.40000
Iteration 145000, batch loss: 0.011205, training accuracy: 0.42000
Iteration 150000, batch loss: 0.010558, training accuracy: 0.38000
Iteration 155000, batch loss: 0.009484, training accuracy: 0.52000
Iteration 160000, batch loss: 0.010041, training accuracy: 0.42000
Iteration 165000, batch loss: 0.009710, training accuracy: 0.50000
Iteration 170000, batch loss: 0.009920, training accuracy: 0.40000
Iteration 175000, batch loss: 0.008922, training accuracy: 0.62000
Iteration 180000, batch loss: 0.009645, training accuracy: 0.48000
Iteration 185000, batch loss: 0.009521, training accuracy: 0.52000
Iteration 190000, batch loss: 0.009307, training accuracy: 0.56000
Iteration 195000, batch loss: 0.008772, training accuracy: 0.46000
Iteration 200000, batch loss: 0.009206, training accuracy: 0.50000
Iteration 205000, batch loss: 0.010502, training accuracy: 0.40000
Iteration 210000, batch loss: 0.009419, training accuracy: 0.46000
Iteration 215000, batch loss: 0.010376, training accuracy: 0.42000
Iteration 220000, batch loss: 0.007734, training accuracy: 0.62000
Iteration 225000, batch loss: 0.009021, training accuracy: 0.54000
Iteration 230000, batch loss: 0.008764, training accuracy: 0.56000
Iteration 235000, batch loss: 0.010162, training accuracy: 0.40000
Iteration 240000, batch loss: 0.009171, training accuracy: 0.48000
Iteration 245000, batch loss: 0.010038, training accuracy: 0.48000
Iteration 250000, batch loss: 0.007701, training accuracy: 0.62000
Iteration 255000, batch loss: 0.009163, training accuracy: 0.48000
Iteration 260000, batch loss: 0.009618, training accuracy: 0.52000
Iteration 265000, batch loss: 0.009375, training accuracy: 0.44000
Iteration 270000, batch loss: 0.008236, training accuracy: 0.58000
Iteration 275000, batch loss: 0.008488, training accuracy: 0.60000
Iteration 280000, batch loss: 0.008604, training accuracy: 0.58000
Iteration 285000, batch loss: 0.008770, training accuracy: 0.52000
Iteration 290000, batch loss: 0.007633, training accuracy: 0.68000
Iteration 295000, batch loss: 0.008490, training accuracy: 0.52000
Iteration 300000, batch loss: 0.008699, training accuracy: 0.54000
Iteration 305000, batch loss: 0.007943, training accuracy: 0.68000
Iteration 310000, batch loss: 0.007744, training accuracy: 0.52000
Iteration 315000, batch loss: 0.008290, training accuracy: 0.56000
Iteration 320000, batch loss: 0.008820, training accuracy: 0.54000
Iteration 325000, batch loss: 0.008788, training accuracy: 0.52000
Iteration 330000, batch loss: 0.009527, training accuracy: 0.50000
Iteration 335000, batch loss: 0.006856, training accuracy: 0.64000
Iteration 340000, batch loss: 0.007045, training accuracy: 0.66000
Iteration 345000, batch loss: 0.008571, training accuracy: 0.64000
Iteration 350000, batch loss: 0.009124, training accuracy: 0.46000
Iteration 355000, batch loss: 0.007775, training accuracy: 0.56000
Iteration 360000, batch loss: 0.008499, training accuracy: 0.62000
Iteration 365000, batch loss: 0.006564, training accuracy: 0.68000
Iteration 370000, batch loss: 0.007773, training accuracy: 0.64000
Iteration 375000, batch loss: 0.008450, training accuracy: 0.62000
Iteration 380000, batch loss: 0.008159, training accuracy: 0.60000
Iteration 385000, batch loss: 0.006514, training accuracy: 0.72000
Iteration 390000, batch loss: 0.007012, training accuracy: 0.66000
Iteration 395000, batch loss: 0.007202, training accuracy: 0.70000
Iteration 400000, batch loss: 0.007786, training accuracy: 0.60000
Iteration 405000, batch loss: 0.006676, training accuracy: 0.72000
Iteration 410000, batch loss: 0.007455, training accuracy: 0.62000
Iteration 415000, batch loss: 0.007054, training accuracy: 0.68000
Iteration 420000, batch loss: 0.006340, training accuracy: 0.70000
Iteration 425000, batch loss: 0.006437, training accuracy: 0.66000
Iteration 430000, batch loss: 0.007260, training accuracy: 0.60000
Iteration 435000, batch loss: 0.006462, training accuracy: 0.70000
Iteration 440000, batch loss: 0.007746, training accuracy: 0.60000
Iteration 445000, batch loss: 0.008083, training accuracy: 0.56000
Iteration 450000, batch loss: 0.005660, training accuracy: 0.68000
Iteration 455000, batch loss: 0.005186, training accuracy: 0.78000
Iteration 460000, batch loss: 0.006759, training accuracy: 0.72000
Iteration 465000, batch loss: 0.007851, training accuracy: 0.60000
Iteration 470000, batch loss: 0.006194, training accuracy: 0.72000
Iteration 475000, batch loss: 0.006633, training accuracy: 0.76000
Iteration 480000, batch loss: 0.005180, training accuracy: 0.76000
Iteration 485000, batch loss: 0.005879, training accuracy: 0.76000
Iteration 490000, batch loss: 0.006517, training accuracy: 0.70000
Iteration 495000, batch loss: 0.006557, training accuracy: 0.76000
Iteration 500000, batch loss: 0.004260, training accuracy: 0.90000
Iteration 505000, batch loss: 0.005366, training accuracy: 0.76000
Iteration 510000, batch loss: 0.005863, training accuracy: 0.74000
Iteration 515000, batch loss: 0.006018, training accuracy: 0.76000
Iteration 520000, batch loss: 0.004920, training accuracy: 0.84000
Iteration 525000, batch loss: 0.005485, training accuracy: 0.78000
Iteration 530000, batch loss: 0.005286, training accuracy: 0.84000
Iteration 535000, batch loss: 0.003989, training accuracy: 0.82000
Iteration 540000, batch loss: 0.005515, training accuracy: 0.74000
Iteration 545000, batch loss: 0.005795, training accuracy: 0.74000
Iteration 550000, batch loss: 0.004962, training accuracy: 0.78000
Iteration 555000, batch loss: 0.005667, training accuracy: 0.72000
Iteration 560000, batch loss: 0.006137, training accuracy: 0.68000
Iteration 565000, batch loss: 0.004640, training accuracy: 0.80000
Iteration 570000, batch loss: 0.003865, training accuracy: 0.88000
Iteration 575000, batch loss: 0.004838, training accuracy: 0.76000
Iteration 580000, batch loss: 0.005973, training accuracy: 0.70000
Iteration 585000, batch loss: 0.004525, training accuracy: 0.80000
Iteration 590000, batch loss: 0.003966, training accuracy: 0.86000
Iteration 595000, batch loss: 0.003447, training accuracy: 0.86000
Iteration 600000, batch loss: 0.004385, training accuracy: 0.80000
Iteration 605000, batch loss: 0.004019, training accuracy: 0.80000
Iteration 610000, batch loss: 0.004509, training accuracy: 0.82000
Iteration 615000, batch loss: 0.002747, training accuracy: 0.92000
Iteration 620000, batch loss: 0.004377, training accuracy: 0.80000
Iteration 625000, batch loss: 0.004119, training accuracy: 0.82000
Iteration 630000, batch loss: 0.004107, training accuracy: 0.84000
Iteration 635000, batch loss: 0.003629, training accuracy: 0.84000
Iteration 640000, batch loss: 0.003756, training accuracy: 0.80000
Iteration 645000, batch loss: 0.003760, training accuracy: 0.84000
Iteration 650000, batch loss: 0.002622, training accuracy: 0.90000
Iteration 655000, batch loss: 0.004389, training accuracy: 0.78000
Iteration 660000, batch loss: 0.003987, training accuracy: 0.88000
Iteration 665000, batch loss: 0.003984, training accuracy: 0.82000
Iteration 670000, batch loss: 0.004217, training accuracy: 0.78000
Iteration 675000, batch loss: 0.004544, training accuracy: 0.74000
Iteration 680000, batch loss: 0.003816, training accuracy: 0.80000
Iteration 685000, batch loss: 0.002648, training accuracy: 0.92000
Iteration 690000, batch loss: 0.003902, training accuracy: 0.82000
Iteration 695000, batch loss: 0.004855, training accuracy: 0.74000
Iteration 700000, batch loss: 0.003549, training accuracy: 0.80000
Iteration 705000, batch loss: 0.003130, training accuracy: 0.90000
Iteration 710000, batch loss: 0.002445, training accuracy: 0.90000
Iteration 715000, batch loss: 0.003354, training accuracy: 0.82000
Iteration 720000, batch loss: 0.002981, training accuracy: 0.84000
Iteration 725000, batch loss: 0.003270, training accuracy: 0.86000
Iteration 730000, batch loss: 0.001835, training accuracy: 0.98000
Iteration 735000, batch loss: 0.002940, training accuracy: 0.88000
Iteration 740000, batch loss: 0.002897, training accuracy: 0.86000
Iteration 745000, batch loss: 0.002863, training accuracy: 0.90000
Iteration 750000, batch loss: 0.002932, training accuracy: 0.88000
Iteration 755000, batch loss: 0.002587, training accuracy: 0.90000
Iteration 760000, batch loss: 0.002688, training accuracy: 0.92000
Iteration 765000, batch loss: 0.001961, training accuracy: 0.90000
Iteration 770000, batch loss: 0.003345, training accuracy: 0.82000
Iteration 775000, batch loss: 0.002594, training accuracy: 0.88000
Iteration 780000, batch loss: 0.002588, training accuracy: 0.86000
Iteration 785000, batch loss: 0.003416, training accuracy: 0.84000
Iteration 790000, batch loss: 0.003506, training accuracy: 0.78000
Iteration 795000, batch loss: 0.003327, training accuracy: 0.82000
Iteration 800000, batch loss: 0.001569, training accuracy: 0.94000
Iteration 805000, batch loss: 0.003078, training accuracy: 0.86000
Iteration 810000, batch loss: 0.003751, training accuracy: 0.84000
Iteration 815000, batch loss: 0.002681, training accuracy: 0.88000
Iteration 820000, batch loss: 0.002044, training accuracy: 0.96000
Iteration 825000, batch loss: 0.001937, training accuracy: 0.96000
Iteration 830000, batch loss: 0.002839, training accuracy: 0.88000
Iteration 835000, batch loss: 0.002407, training accuracy: 0.92000
Iteration 840000, batch loss: 0.002708, training accuracy: 0.88000
Iteration 845000, batch loss: 0.001246, training accuracy: 1.00000
Iteration 850000, batch loss: 0.002205, training accuracy: 0.94000
Iteration 855000, batch loss: 0.002367, training accuracy: 0.88000
Iteration 860000, batch loss: 0.001979, training accuracy: 0.96000
Iteration 865000, batch loss: 0.002041, training accuracy: 0.90000
Iteration 870000, batch loss: 0.002075, training accuracy: 0.90000
Iteration 875000, batch loss: 0.001648, training accuracy: 0.98000
Iteration 880000, batch loss: 0.001848, training accuracy: 0.90000
Iteration 885000, batch loss: 0.002828, training accuracy: 0.90000
Iteration 890000, batch loss: 0.001819, training accuracy: 0.94000
Iteration 895000, batch loss: 0.001970, training accuracy: 0.96000
Iteration 900000, batch loss: 0.002675, training accuracy: 0.86000
Iteration 905000, batch loss: 0.002662, training accuracy: 0.90000
Iteration 910000, batch loss: 0.002692, training accuracy: 0.88000
Iteration 915000, batch loss: 0.001451, training accuracy: 0.96000
Iteration 920000, batch loss: 0.002367, training accuracy: 0.92000
Iteration 925000, batch loss: 0.002829, training accuracy: 0.88000
Iteration 930000, batch loss: 0.001949, training accuracy: 0.92000
Iteration 935000, batch loss: 0.001475, training accuracy: 0.94000
Iteration 940000, batch loss: 0.001481, training accuracy: 0.96000
Iteration 945000, batch loss: 0.002301, training accuracy: 0.96000
Iteration 950000, batch loss: 0.002028, training accuracy: 0.94000
Iteration 955000, batch loss: 0.001957, training accuracy: 0.94000
Iteration 960000, batch loss: 0.000632, training accuracy: 1.00000
Iteration 965000, batch loss: 0.001603, training accuracy: 0.92000
Iteration 970000, batch loss: 0.001963, training accuracy: 0.90000
Iteration 975000, batch loss: 0.001418, training accuracy: 0.96000
Iteration 980000, batch loss: 0.002178, training accuracy: 0.90000
Iteration 985000, batch loss: 0.001546, training accuracy: 0.94000
Iteration 990000, batch loss: 0.001290, training accuracy: 0.98000
Iteration 995000, batch loss: 0.001520, training accuracy: 0.96000
Iteration 1000000, batch loss: 0.001951, training accuracy: 0.96000
Iteration 1005000, batch loss: 0.001470, training accuracy: 0.96000
Iteration 1010000, batch loss: 0.001609, training accuracy: 0.94000
Iteration 1015000, batch loss: 0.002361, training accuracy: 0.92000
Iteration 1020000, batch loss: 0.001806, training accuracy: 0.96000
Iteration 1025000, batch loss: 0.002079, training accuracy: 0.90000
Iteration 1030000, batch loss: 0.001189, training accuracy: 0.98000
Iteration 1035000, batch loss: 0.001929, training accuracy: 0.92000
Iteration 1040000, batch loss: 0.002181, training accuracy: 0.90000
Iteration 1045000, batch loss: 0.001419, training accuracy: 0.94000
Iteration 1050000, batch loss: 0.001183, training accuracy: 0.96000
Iteration 1055000, batch loss: 0.001201, training accuracy: 0.96000
Iteration 1060000, batch loss: 0.001537, training accuracy: 0.98000
Iteration 1065000, batch loss: 0.001305, training accuracy: 0.98000
Iteration 1070000, batch loss: 0.001721, training accuracy: 0.92000
Iteration 1075000, batch loss: 0.000552, training accuracy: 1.00000
Iteration 1080000, batch loss: 0.001175, training accuracy: 1.00000
Iteration 1085000, batch loss: 0.001674, training accuracy: 0.94000
Iteration 1090000, batch loss: 0.000989, training accuracy: 0.96000
Iteration 1095000, batch loss: 0.001716, training accuracy: 0.92000
Iteration 1100000, batch loss: 0.001363, training accuracy: 0.94000
Iteration 1105000, batch loss: 0.000569, training accuracy: 1.00000
Iteration 1110000, batch loss: 0.000915, training accuracy: 0.96000
Iteration 1115000, batch loss: 0.001182, training accuracy: 0.94000
Iteration 1120000, batch loss: 0.000951, training accuracy: 0.98000
Iteration 1125000, batch loss: 0.000989, training accuracy: 0.96000
Iteration 1130000, batch loss: 0.001626, training accuracy: 0.94000
Iteration 1135000, batch loss: 0.001403, training accuracy: 0.98000
Iteration 1140000, batch loss: 0.001683, training accuracy: 0.92000
Iteration 1145000, batch loss: 0.000854, training accuracy: 0.98000
Iteration 1150000, batch loss: 0.001598, training accuracy: 0.94000
Iteration 1155000, batch loss: 0.001628, training accuracy: 0.94000
Iteration 1160000, batch loss: 0.001217, training accuracy: 0.94000
Iteration 1165000, batch loss: 0.000809, training accuracy: 1.00000
Iteration 1170000, batch loss: 0.000935, training accuracy: 0.96000
Iteration 1175000, batch loss: 0.000810, training accuracy: 0.98000
Iteration 1180000, batch loss: 0.000703, training accuracy: 1.00000
Iteration 1185000, batch loss: 0.001315, training accuracy: 0.96000
Iteration 1190000, batch loss: 0.000396, training accuracy: 1.00000
Iteration 1195000, batch loss: 0.000649, training accuracy: 1.00000
Iteration 1200000, batch loss: 0.001373, training accuracy: 0.94000
Iteration 1205000, batch loss: 0.000866, training accuracy: 0.98000
Iteration 1210000, batch loss: 0.001426, training accuracy: 0.96000
Iteration 1215000, batch loss: 0.001091, training accuracy: 0.94000
Iteration 1220000, batch loss: 0.000499, training accuracy: 1.00000
Iteration 1225000, batch loss: 0.000812, training accuracy: 0.98000
Iteration 1230000, batch loss: 0.000600, training accuracy: 1.00000
Iteration 1235000, batch loss: 0.000746, training accuracy: 0.98000
Iteration 1240000, batch loss: 0.000883, training accuracy: 0.96000
Iteration 1245000, batch loss: 0.001370, training accuracy: 0.94000
Iteration 1250000, batch loss: 0.000943, training accuracy: 0.98000
Iteration 1255000, batch loss: 0.001110, training accuracy: 0.94000
Iteration 1260000, batch loss: 0.000617, training accuracy: 0.98000
Iteration 1265000, batch loss: 0.002702, training accuracy: 0.90000
Iteration 1270000, batch loss: 0.001163, training accuracy: 0.98000
Iteration 1275000, batch loss: 0.001004, training accuracy: 0.98000
Iteration 1280000, batch loss: 0.000440, training accuracy: 1.00000
Iteration 1285000, batch loss: 0.000850, training accuracy: 0.98000
Iteration 1290000, batch loss: 0.000693, training accuracy: 0.98000
Iteration 1295000, batch loss: 0.000657, training accuracy: 1.00000
Iteration 1300000, batch loss: 0.001104, training accuracy: 0.96000
Iteration 1305000, batch loss: 0.000376, training accuracy: 1.00000
Iteration 1310000, batch loss: 0.000436, training accuracy: 1.00000
Iteration 1315000, batch loss: 0.001244, training accuracy: 0.98000
Iteration 1320000, batch loss: 0.000673, training accuracy: 0.98000
Iteration 1325000, batch loss: 0.001216, training accuracy: 0.96000
Iteration 1330000, batch loss: 0.001012, training accuracy: 0.94000
Iteration 1335000, batch loss: 0.000436, training accuracy: 1.00000
Iteration 1340000, batch loss: 0.000602, training accuracy: 1.00000
Iteration 1345000, batch loss: 0.000388, training accuracy: 1.00000
Iteration 1350000, batch loss: 0.000569, training accuracy: 0.98000
Iteration 1355000, batch loss: 0.000619, training accuracy: 0.98000
Iteration 1360000, batch loss: 0.001021, training accuracy: 0.98000
Iteration 1365000, batch loss: 0.000696, training accuracy: 0.98000
Iteration 1370000, batch loss: 0.000795, training accuracy: 0.96000
Iteration 1375000, batch loss: 0.000691, training accuracy: 0.98000
Iteration 1380000, batch loss: 0.000852, training accuracy: 0.98000
Iteration 1385000, batch loss: 0.001042, training accuracy: 0.98000
Iteration 1390000, batch loss: 0.000788, training accuracy: 0.98000
Iteration 1395000, batch loss: 0.000306, training accuracy: 1.00000
Iteration 1400000, batch loss: 0.000675, training accuracy: 0.98000
Iteration 1405000, batch loss: 0.000619, training accuracy: 0.98000
Iteration 1410000, batch loss: 0.000472, training accuracy: 1.00000
Iteration 1415000, batch loss: 0.001065, training accuracy: 0.98000
Iteration 1420000, batch loss: 0.000245, training accuracy: 1.00000
Iteration 1425000, batch loss: 0.000344, training accuracy: 1.00000
Iteration 1430000, batch loss: 0.000922, training accuracy: 0.98000
Iteration 1435000, batch loss: 0.000544, training accuracy: 0.98000
Iteration 1440000, batch loss: 0.000709, training accuracy: 1.00000
Iteration 1445000, batch loss: 0.000949, training accuracy: 0.94000
Iteration 1450000, batch loss: 0.000298, training accuracy: 1.00000
Iteration 1455000, batch loss: 0.000326, training accuracy: 1.00000
Iteration 1460000, batch loss: 0.000390, training accuracy: 1.00000
Iteration 1465000, batch loss: 0.000448, training accuracy: 1.00000
Iteration 1470000, batch loss: 0.000555, training accuracy: 0.98000
Iteration 1475000, batch loss: 0.000814, training accuracy: 1.00000
Iteration 1480000, batch loss: 0.000570, training accuracy: 1.00000
Iteration 1485000, batch loss: 0.000870, training accuracy: 0.96000
Iteration 1490000, batch loss: 0.000482, training accuracy: 0.98000
Iteration 1495000, batch loss: 0.000638, training accuracy: 0.98000
Iteration 1500000, batch loss: 0.000770, training accuracy: 0.98000
Iteration 1505000, batch loss: 0.000787, training accuracy: 0.98000
Iteration 1510000, batch loss: 0.000257, training accuracy: 1.00000
Iteration 1515000, batch loss: 0.000550, training accuracy: 0.98000
Iteration 1520000, batch loss: 0.000476, training accuracy: 0.98000
Iteration 1525000, batch loss: 0.000292, training accuracy: 1.00000
Iteration 1530000, batch loss: 0.000684, training accuracy: 1.00000
Iteration 1535000, batch loss: 0.000291, training accuracy: 1.00000
Iteration 1540000, batch loss: 0.000287, training accuracy: 1.00000
Iteration 1545000, batch loss: 0.000892, training accuracy: 0.98000
Iteration 1550000, batch loss: 0.000467, training accuracy: 0.98000
Iteration 1555000, batch loss: 0.000677, training accuracy: 0.98000
Iteration 1560000, batch loss: 0.000900, training accuracy: 0.96000
Iteration 1565000, batch loss: 0.000176, training accuracy: 1.00000
Iteration 1570000, batch loss: 0.000342, training accuracy: 1.00000
Iteration 1575000, batch loss: 0.000254, training accuracy: 1.00000
Iteration 1580000, batch loss: 0.000469, training accuracy: 0.98000
Iteration 1585000, batch loss: 0.000461, training accuracy: 0.98000
Iteration 1590000, batch loss: 0.000682, training accuracy: 1.00000
Iteration 1595000, batch loss: 0.000640, training accuracy: 1.00000
Iteration 1600000, batch loss: 0.000796, training accuracy: 0.96000
Iteration 1605000, batch loss: 0.000523, training accuracy: 0.98000
Iteration 1610000, batch loss: 0.000735, training accuracy: 0.98000
Iteration 1615000, batch loss: 0.000821, training accuracy: 0.98000
Iteration 1620000, batch loss: 0.000720, training accuracy: 0.98000
Iteration 1625000, batch loss: 0.000301, training accuracy: 1.00000
Iteration 1630000, batch loss: 0.000648, training accuracy: 0.98000
Iteration 1635000, batch loss: 0.000601, training accuracy: 0.98000
Iteration 1640000, batch loss: 0.000340, training accuracy: 1.00000
Iteration 1645000, batch loss: 0.000736, training accuracy: 0.98000
Iteration 1650000, batch loss: 0.000413, training accuracy: 1.00000
Iteration 1655000, batch loss: 0.000339, training accuracy: 1.00000
Iteration 1660000, batch loss: 0.000887, training accuracy: 0.98000
Iteration 1665000, batch loss: 0.000496, training accuracy: 0.98000
Iteration 1670000, batch loss: 0.000560, training accuracy: 1.00000
Iteration 1675000, batch loss: 0.000844, training accuracy: 1.00000
Iteration 1680000, batch loss: 0.000127, training accuracy: 1.00000
Iteration 1685000, batch loss: 0.000181, training accuracy: 1.00000
Iteration 1690000, batch loss: 0.000119, training accuracy: 1.00000
Iteration 1695000, batch loss: 0.000303, training accuracy: 0.98000
Iteration 1700000, batch loss: 0.000316, training accuracy: 1.00000
Iteration 1705000, batch loss: 0.000289, training accuracy: 1.00000
Iteration 1710000, batch loss: 0.000210, training accuracy: 1.00000
Iteration 1715000, batch loss: 0.000649, training accuracy: 0.96000
Iteration 1720000, batch loss: 0.000390, training accuracy: 1.00000
Iteration 1725000, batch loss: 0.000467, training accuracy: 1.00000
Iteration 1730000, batch loss: 0.000468, training accuracy: 0.98000
Iteration 1735000, batch loss: 0.000466, training accuracy: 0.98000
Iteration 1740000, batch loss: 0.000175, training accuracy: 1.00000
Iteration 1745000, batch loss: 0.000405, training accuracy: 0.98000
Iteration 1750000, batch loss: 0.000431, training accuracy: 0.98000
Iteration 1755000, batch loss: 0.000265, training accuracy: 1.00000
Iteration 1760000, batch loss: 0.000380, training accuracy: 1.00000
Iteration 1765000, batch loss: 0.000123, training accuracy: 1.00000
Iteration 1770000, batch loss: 0.000199, training accuracy: 1.00000
Iteration 1775000, batch loss: 0.000681, training accuracy: 0.98000
Iteration 1780000, batch loss: 0.000426, training accuracy: 0.98000
Iteration 1785000, batch loss: 0.000553, training accuracy: 0.98000
Iteration 1790000, batch loss: 0.000809, training accuracy: 0.96000
Iteration 1795000, batch loss: 0.000156, training accuracy: 1.00000
Iteration 1800000, batch loss: 0.000173, training accuracy: 1.00000
Iteration 1805000, batch loss: 0.000325, training accuracy: 1.00000
Iteration 1810000, batch loss: 0.000294, training accuracy: 0.98000
Iteration 1815000, batch loss: 0.000290, training accuracy: 1.00000
Iteration 1820000, batch loss: 0.000221, training accuracy: 1.00000
Iteration 1825000, batch loss: 0.000331, training accuracy: 1.00000
Iteration 1830000, batch loss: 0.000638, training accuracy: 0.96000
Iteration 1835000, batch loss: 0.000376, training accuracy: 1.00000
Iteration 1840000, batch loss: 0.000487, training accuracy: 0.98000
Iteration 1845000, batch loss: 0.000650, training accuracy: 0.98000
Iteration 1850000, batch loss: 0.000507, training accuracy: 0.98000
Iteration 1855000, batch loss: 0.000234, training accuracy: 1.00000
Iteration 1860000, batch loss: 0.000356, training accuracy: 1.00000
Iteration 1865000, batch loss: 0.000402, training accuracy: 0.98000
Iteration 1870000, batch loss: 0.000279, training accuracy: 1.00000
Iteration 1875000, batch loss: 0.000317, training accuracy: 1.00000
Iteration 1880000, batch loss: 0.000114, training accuracy: 1.00000
Iteration 1885000, batch loss: 0.000104, training accuracy: 1.00000
Iteration 1890000, batch loss: 0.000566, training accuracy: 1.00000
Iteration 1895000, batch loss: 0.000380, training accuracy: 0.98000
Iteration 1900000, batch loss: 0.000459, training accuracy: 1.00000
Iteration 1905000, batch loss: 0.000752, training accuracy: 0.96000
Iteration 1910000, batch loss: 0.000265, training accuracy: 1.00000
Iteration 1915000, batch loss: 0.000174, training accuracy: 1.00000
Iteration 1920000, batch loss: 0.000162, training accuracy: 1.00000
Iteration 1925000, batch loss: 0.000305, training accuracy: 1.00000
Iteration 1930000, batch loss: 0.000386, training accuracy: 1.00000
Iteration 1935000, batch loss: 0.000309, training accuracy: 1.00000
Iteration 1940000, batch loss: 0.000323, training accuracy: 1.00000
Iteration 1945000, batch loss: 0.000624, training accuracy: 1.00000
Iteration 1950000, batch loss: 0.000453, training accuracy: 0.98000
Iteration 1955000, batch loss: 0.000651, training accuracy: 0.98000
Iteration 1960000, batch loss: 0.000456, training accuracy: 0.98000
Iteration 1965000, batch loss: 0.000440, training accuracy: 0.98000
Iteration 1970000, batch loss: 0.000166, training accuracy: 1.00000
Iteration 1975000, batch loss: 0.000224, training accuracy: 1.00000
Iteration 1980000, batch loss: 0.000254, training accuracy: 1.00000
Iteration 1985000, batch loss: 0.000098, training accuracy: 1.00000
Iteration 1990000, batch loss: 0.000323, training accuracy: 1.00000
Iteration 1995000, batch loss: 0.000112, training accuracy: 1.00000
Iteration 2000000, batch loss: 0.000148, training accuracy: 1.00000
Iteration 2005000, batch loss: 0.000362, training accuracy: 1.00000
Iteration 2010000, batch loss: 0.000390, training accuracy: 0.98000
Iteration 2015000, batch loss: 0.000487, training accuracy: 1.00000
Iteration 2020000, batch loss: 0.000685, training accuracy: 0.96000
Iteration 2025000, batch loss: 0.000160, training accuracy: 1.00000
Iteration 2030000, batch loss: 0.000099, training accuracy: 1.00000
Iteration 2035000, batch loss: 0.000105, training accuracy: 1.00000
Iteration 2040000, batch loss: 0.000200, training accuracy: 1.00000
Iteration 2045000, batch loss: 0.000171, training accuracy: 1.00000
Iteration 2050000, batch loss: 0.000230, training accuracy: 1.00000
Iteration 2055000, batch loss: 0.000181, training accuracy: 1.00000
Iteration 2060000, batch loss: 0.000375, training accuracy: 1.00000
Iteration 2065000, batch loss: 0.000361, training accuracy: 0.98000
Iteration 2070000, batch loss: 0.000382, training accuracy: 0.98000
Iteration 2075000, batch loss: 0.000360, training accuracy: 0.98000
Iteration 2080000, batch loss: 0.000357, training accuracy: 1.00000
Iteration 2085000, batch loss: 0.000090, training accuracy: 1.00000
Iteration 2090000, batch loss: 0.000141, training accuracy: 1.00000
Iteration 2095000, batch loss: 0.000161, training accuracy: 1.00000
Iteration 2100000, batch loss: 0.000110, training accuracy: 1.00000
Iteration 2105000, batch loss: 0.000210, training accuracy: 1.00000
Iteration 2110000, batch loss: 0.000091, training accuracy: 1.00000
Iteration 2115000, batch loss: 0.000090, training accuracy: 1.00000
Iteration 2120000, batch loss: 0.000190, training accuracy: 1.00000
