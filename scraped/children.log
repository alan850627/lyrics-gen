Iteration 10000, batch loss: 0.015702, training accuracy: 0.08000
Iteration 20000, batch loss: 0.013273, training accuracy: 0.20000
Iteration 30000, batch loss: 0.012552, training accuracy: 0.20000
Iteration 40000, batch loss: 0.012016, training accuracy: 0.16000
Iteration 50000, batch loss: 0.012012, training accuracy: 0.26000
Iteration 60000, batch loss: 0.011346, training accuracy: 0.24000
Iteration 70000, batch loss: 0.011458, training accuracy: 0.32000
Iteration 80000, batch loss: 0.011256, training accuracy: 0.40000
Iteration 90000, batch loss: 0.010567, training accuracy: 0.36000
Iteration 100000, batch loss: 0.011330, training accuracy: 0.32000
Iteration 110000, batch loss: 0.010629, training accuracy: 0.34000
Iteration 120000, batch loss: 0.009929, training accuracy: 0.42000
Iteration 130000, batch loss: 0.011165, training accuracy: 0.28000
Iteration 140000, batch loss: 0.010250, training accuracy: 0.36000
Iteration 150000, batch loss: 0.010699, training accuracy: 0.32000
Iteration 160000, batch loss: 0.009935, training accuracy: 0.36000
Iteration 170000, batch loss: 0.010071, training accuracy: 0.32000
Iteration 180000, batch loss: 0.009145, training accuracy: 0.46000
Iteration 190000, batch loss: 0.009411, training accuracy: 0.44000
Iteration 200000, batch loss: 0.010558, training accuracy: 0.40000
Iteration 210000, batch loss: 0.010964, training accuracy: 0.30000
Iteration 220000, batch loss: 0.010930, training accuracy: 0.32000
Iteration 230000, batch loss: 0.009768, training accuracy: 0.42000
Iteration 240000, batch loss: 0.009321, training accuracy: 0.50000
Iteration 250000, batch loss: 0.008784, training accuracy: 0.50000
Iteration 260000, batch loss: 0.008687, training accuracy: 0.56000
Iteration 270000, batch loss: 0.010132, training accuracy: 0.38000
Iteration 280000, batch loss: 0.010105, training accuracy: 0.36000
Iteration 290000, batch loss: 0.009792, training accuracy: 0.40000
Iteration 300000, batch loss: 0.008865, training accuracy: 0.46000
Iteration 310000, batch loss: 0.009806, training accuracy: 0.42000
Iteration 320000, batch loss: 0.008285, training accuracy: 0.54000
Iteration 330000, batch loss: 0.009324, training accuracy: 0.50000
Iteration 340000, batch loss: 0.008460, training accuracy: 0.56000
Iteration 350000, batch loss: 0.009041, training accuracy: 0.42000
Iteration 360000, batch loss: 0.008328, training accuracy: 0.46000
Iteration 370000, batch loss: 0.008499, training accuracy: 0.58000
Iteration 380000, batch loss: 0.009019, training accuracy: 0.44000
Iteration 390000, batch loss: 0.001175, training accuracy: 0.96000
Iteration 400000, batch loss: 0.009313, training accuracy: 0.40000
Iteration 410000, batch loss: 0.008363, training accuracy: 0.56000
Iteration 420000, batch loss: 0.009096, training accuracy: 0.48000
Iteration 430000, batch loss: 0.008542, training accuracy: 0.44000
Iteration 440000, batch loss: 0.007136, training accuracy: 0.62000
Iteration 450000, batch loss: 0.007395, training accuracy: 0.70000
Iteration 460000, batch loss: 0.007604, training accuracy: 0.62000
Iteration 470000, batch loss: 0.008406, training accuracy: 0.50000
Iteration 480000, batch loss: 0.006205, training accuracy: 0.78000
Iteration 490000, batch loss: 0.008338, training accuracy: 0.54000
Iteration 500000, batch loss: 0.007566, training accuracy: 0.62000
Iteration 510000, batch loss: 0.006504, training accuracy: 0.64000
Iteration 520000, batch loss: 0.007507, training accuracy: 0.64000
Iteration 530000, batch loss: 0.006028, training accuracy: 0.72000
Iteration 540000, batch loss: 0.006660, training accuracy: 0.70000
Iteration 550000, batch loss: 0.005976, training accuracy: 0.72000
Iteration 560000, batch loss: 0.005890, training accuracy: 0.74000
Iteration 570000, batch loss: 0.004999, training accuracy: 0.82000
Iteration 580000, batch loss: 0.005417, training accuracy: 0.70000
Iteration 590000, batch loss: 0.006737, training accuracy: 0.60000
Iteration 600000, batch loss: 0.006610, training accuracy: 0.76000
Iteration 610000, batch loss: 0.006275, training accuracy: 0.72000
Iteration 620000, batch loss: 0.004956, training accuracy: 0.80000
Iteration 630000, batch loss: 0.005253, training accuracy: 0.72000
Iteration 640000, batch loss: 0.004905, training accuracy: 0.78000
Iteration 650000, batch loss: 0.004405, training accuracy: 0.82000
Iteration 660000, batch loss: 0.005310, training accuracy: 0.72000
Iteration 670000, batch loss: 0.005030, training accuracy: 0.78000
Iteration 680000, batch loss: 0.005144, training accuracy: 0.72000
Iteration 690000, batch loss: 0.004085, training accuracy: 0.86000
Iteration 700000, batch loss: 0.002978, training accuracy: 0.94000
Iteration 710000, batch loss: 0.003838, training accuracy: 0.80000
Iteration 720000, batch loss: 0.003895, training accuracy: 0.80000
Iteration 730000, batch loss: 0.002773, training accuracy: 0.92000
Iteration 740000, batch loss: 0.002987, training accuracy: 0.88000
Iteration 750000, batch loss: 0.003536, training accuracy: 0.86000
Iteration 760000, batch loss: 0.003793, training accuracy: 0.80000
Iteration 770000, batch loss: 0.004250, training accuracy: 0.82000
Iteration 780000, batch loss: 0.000125, training accuracy: 1.00000
Iteration 790000, batch loss: 0.003677, training accuracy: 0.84000
Iteration 800000, batch loss: 0.003113, training accuracy: 0.84000
Iteration 810000, batch loss: 0.003422, training accuracy: 0.90000
Iteration 820000, batch loss: 0.003318, training accuracy: 0.92000
Iteration 830000, batch loss: 0.003530, training accuracy: 0.78000
Iteration 840000, batch loss: 0.002897, training accuracy: 0.90000
Iteration 850000, batch loss: 0.002697, training accuracy: 0.90000
Iteration 860000, batch loss: 0.003256, training accuracy: 0.84000
Iteration 870000, batch loss: 0.001369, training accuracy: 0.98000
Iteration 880000, batch loss: 0.003056, training accuracy: 0.96000
Iteration 890000, batch loss: 0.002919, training accuracy: 0.88000
Iteration 900000, batch loss: 0.002600, training accuracy: 0.86000
Iteration 910000, batch loss: 0.002260, training accuracy: 0.90000
Iteration 920000, batch loss: 0.001920, training accuracy: 0.92000
Iteration 930000, batch loss: 0.002237, training accuracy: 0.94000
Iteration 940000, batch loss: 0.002132, training accuracy: 0.92000
Iteration 950000, batch loss: 0.002554, training accuracy: 0.88000
Iteration 960000, batch loss: 0.001513, training accuracy: 0.96000
Iteration 970000, batch loss: 0.001368, training accuracy: 0.98000
Iteration 980000, batch loss: 0.002819, training accuracy: 0.90000
Iteration 990000, batch loss: 0.001800, training accuracy: 0.94000
Iteration 1000000, batch loss: 0.002354, training accuracy: 0.90000
Iteration 1010000, batch loss: 0.002053, training accuracy: 0.96000
Iteration 1020000, batch loss: 0.000926, training accuracy: 1.00000
Iteration 1030000, batch loss: 0.001594, training accuracy: 0.92000
Iteration 1040000, batch loss: 0.001408, training accuracy: 0.96000
Iteration 1050000, batch loss: 0.002358, training accuracy: 0.90000
Iteration 1060000, batch loss: 0.001419, training accuracy: 0.98000
Iteration 1070000, batch loss: 0.001271, training accuracy: 0.98000
Iteration 1080000, batch loss: 0.001286, training accuracy: 0.94000
Iteration 1090000, batch loss: 0.000850, training accuracy: 1.00000
Iteration 1100000, batch loss: 0.001534, training accuracy: 0.92000
Iteration 1110000, batch loss: 0.000909, training accuracy: 1.00000
Iteration 1120000, batch loss: 0.000875, training accuracy: 0.98000
Iteration 1130000, batch loss: 0.001009, training accuracy: 0.98000
Iteration 1140000, batch loss: 0.000716, training accuracy: 1.00000
Iteration 1150000, batch loss: 0.001865, training accuracy: 0.94000
Iteration 1160000, batch loss: 0.001641, training accuracy: 0.92000
Iteration 1170000, batch loss: 0.000241, training accuracy: 1.00000
