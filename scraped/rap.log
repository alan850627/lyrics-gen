Iteration 10000, batch loss: 0.010415, training accuracy: 0.24000
Iteration 20000, batch loss: 0.010148, training accuracy: 0.20000
Iteration 30000, batch loss: 0.009791, training accuracy: 0.28000
Iteration 40000, batch loss: 0.010153, training accuracy: 0.20000
Iteration 50000, batch loss: 0.009980, training accuracy: 0.22000
Iteration 60000, batch loss: 0.009484, training accuracy: 0.36000
Iteration 70000, batch loss: 0.009448, training accuracy: 0.24000
Iteration 80000, batch loss: 0.009621, training accuracy: 0.26000
Iteration 90000, batch loss: 0.009076, training accuracy: 0.34000
Iteration 100000, batch loss: 0.009673, training accuracy: 0.24000
Iteration 110000, batch loss: 0.009743, training accuracy: 0.28000
Iteration 120000, batch loss: 0.008992, training accuracy: 0.34000
Iteration 130000, batch loss: 0.009211, training accuracy: 0.24000
Iteration 140000, batch loss: 0.008574, training accuracy: 0.38000
Iteration 150000, batch loss: 0.009552, training accuracy: 0.30000
Iteration 160000, batch loss: 0.008969, training accuracy: 0.36000
Iteration 170000, batch loss: 0.008204, training accuracy: 0.40000
Iteration 180000, batch loss: 0.009026, training accuracy: 0.32000
Iteration 190000, batch loss: 0.009027, training accuracy: 0.34000
Iteration 200000, batch loss: 0.009052, training accuracy: 0.30000
Iteration 210000, batch loss: 0.008295, training accuracy: 0.42000
Iteration 220000, batch loss: 0.007381, training accuracy: 0.58000
Iteration 230000, batch loss: 0.009183, training accuracy: 0.28000
Iteration 240000, batch loss: 0.009000, training accuracy: 0.40000
Iteration 250000, batch loss: 0.009363, training accuracy: 0.24000
Iteration 260000, batch loss: 0.007496, training accuracy: 0.42000
Iteration 270000, batch loss: 0.008409, training accuracy: 0.44000
Iteration 280000, batch loss: 0.008040, training accuracy: 0.38000
Iteration 290000, batch loss: 0.007892, training accuracy: 0.38000
Iteration 300000, batch loss: 0.008436, training accuracy: 0.38000
Iteration 310000, batch loss: 0.008588, training accuracy: 0.36000
Iteration 320000, batch loss: 0.008535, training accuracy: 0.34000
Iteration 330000, batch loss: 0.008603, training accuracy: 0.36000
Iteration 340000, batch loss: 0.008487, training accuracy: 0.32000
Iteration 350000, batch loss: 0.007674, training accuracy: 0.44000
Iteration 360000, batch loss: 0.008289, training accuracy: 0.30000
Iteration 370000, batch loss: 0.008492, training accuracy: 0.34000
Iteration 380000, batch loss: 0.007326, training accuracy: 0.48000
Iteration 390000, batch loss: 0.008180, training accuracy: 0.38000
Iteration 400000, batch loss: 0.007625, training accuracy: 0.44000
Iteration 410000, batch loss: 0.008303, training accuracy: 0.40000
Iteration 420000, batch loss: 0.007738, training accuracy: 0.44000
Iteration 430000, batch loss: 0.007059, training accuracy: 0.54000
Iteration 440000, batch loss: 0.007115, training accuracy: 0.52000
Iteration 450000, batch loss: 0.007670, training accuracy: 0.46000
Iteration 460000, batch loss: 0.008655, training accuracy: 0.32000
Iteration 470000, batch loss: 0.007240, training accuracy: 0.52000
Iteration 480000, batch loss: 0.008244, training accuracy: 0.44000
Iteration 490000, batch loss: 0.007944, training accuracy: 0.50000
Iteration 500000, batch loss: 0.007242, training accuracy: 0.48000
Iteration 510000, batch loss: 0.006785, training accuracy: 0.50000
Iteration 520000, batch loss: 0.008177, training accuracy: 0.38000
Iteration 530000, batch loss: 0.008189, training accuracy: 0.34000
Iteration 540000, batch loss: 0.008994, training accuracy: 0.30000
Iteration 550000, batch loss: 0.007547, training accuracy: 0.48000
Iteration 560000, batch loss: 0.006665, training accuracy: 0.50000
Iteration 570000, batch loss: 0.006609, training accuracy: 0.60000
Iteration 580000, batch loss: 0.008035, training accuracy: 0.44000
Iteration 590000, batch loss: 0.008416, training accuracy: 0.44000
Iteration 600000, batch loss: 0.007828, training accuracy: 0.44000
Iteration 610000, batch loss: 0.007347, training accuracy: 0.46000
Iteration 620000, batch loss: 0.008083, training accuracy: 0.40000
Iteration 630000, batch loss: 0.006579, training accuracy: 0.48000
Iteration 640000, batch loss: 0.008652, training accuracy: 0.44000
Iteration 650000, batch loss: 0.008355, training accuracy: 0.56000
Iteration 660000, batch loss: 0.008123, training accuracy: 0.46000
Iteration 670000, batch loss: 0.008272, training accuracy: 0.44000
Iteration 680000, batch loss: 0.007528, training accuracy: 0.50000
Iteration 690000, batch loss: 0.007907, training accuracy: 0.40000
Iteration 700000, batch loss: 0.008376, training accuracy: 0.40000
Iteration 710000, batch loss: 0.007292, training accuracy: 0.58000
Iteration 720000, batch loss: 0.008607, training accuracy: 0.38000
Iteration 730000, batch loss: 0.007563, training accuracy: 0.48000
Iteration 740000, batch loss: 0.008805, training accuracy: 0.32000
Iteration 750000, batch loss: 0.008105, training accuracy: 0.42000
Iteration 760000, batch loss: 0.007598, training accuracy: 0.44000
Iteration 770000, batch loss: 0.007372, training accuracy: 0.44000
Iteration 780000, batch loss: 0.007390, training accuracy: 0.50000
Iteration 790000, batch loss: 0.007452, training accuracy: 0.50000
Iteration 800000, batch loss: 0.007319, training accuracy: 0.54000
Iteration 810000, batch loss: 0.007838, training accuracy: 0.38000
Iteration 820000, batch loss: 0.007606, training accuracy: 0.40000
Iteration 830000, batch loss: 0.007866, training accuracy: 0.46000
Iteration 840000, batch loss: 0.008212, training accuracy: 0.42000
Iteration 850000, batch loss: 0.006990, training accuracy: 0.54000
Iteration 860000, batch loss: 0.007685, training accuracy: 0.42000
Iteration 870000, batch loss: 0.007198, training accuracy: 0.44000
Iteration 880000, batch loss: 0.008633, training accuracy: 0.30000
Iteration 890000, batch loss: 0.007159, training accuracy: 0.46000
Iteration 900000, batch loss: 0.008189, training accuracy: 0.44000
Iteration 910000, batch loss: 0.007481, training accuracy: 0.46000
Iteration 920000, batch loss: 0.007071, training accuracy: 0.42000
Iteration 930000, batch loss: 0.007654, training accuracy: 0.38000
Iteration 940000, batch loss: 0.007666, training accuracy: 0.44000
Iteration 950000, batch loss: 0.007665, training accuracy: 0.48000
Iteration 960000, batch loss: 0.007234, training accuracy: 0.44000
Iteration 970000, batch loss: 0.006446, training accuracy: 0.52000
Iteration 980000, batch loss: 0.007095, training accuracy: 0.52000
Iteration 990000, batch loss: 0.007419, training accuracy: 0.48000
Iteration 1000000, batch loss: 0.006554, training accuracy: 0.52000
Iteration 1010000, batch loss: 0.007567, training accuracy: 0.42000
Iteration 1020000, batch loss: 0.007716, training accuracy: 0.42000
Iteration 1030000, batch loss: 0.007142, training accuracy: 0.52000
Iteration 1040000, batch loss: 0.006936, training accuracy: 0.52000
Iteration 1050000, batch loss: 0.007444, training accuracy: 0.44000
Iteration 1060000, batch loss: 0.006925, training accuracy: 0.48000
Iteration 1070000, batch loss: 0.006272, training accuracy: 0.52000
Iteration 1080000, batch loss: 0.007617, training accuracy: 0.46000
Iteration 1090000, batch loss: 0.007141, training accuracy: 0.52000
Iteration 1100000, batch loss: 0.007786, training accuracy: 0.42000
Iteration 1110000, batch loss: 0.008124, training accuracy: 0.36000
Iteration 1120000, batch loss: 0.006385, training accuracy: 0.50000
Iteration 1130000, batch loss: 0.007368, training accuracy: 0.44000
Iteration 1140000, batch loss: 0.007985, training accuracy: 0.38000
Iteration 1150000, batch loss: 0.008480, training accuracy: 0.42000
Iteration 1160000, batch loss: 0.007784, training accuracy: 0.34000
Iteration 1170000, batch loss: 0.006513, training accuracy: 0.56000
Iteration 1180000, batch loss: 0.007052, training accuracy: 0.56000
Iteration 1190000, batch loss: 0.007844, training accuracy: 0.46000
Iteration 1200000, batch loss: 0.006775, training accuracy: 0.50000
Iteration 1210000, batch loss: 0.007029, training accuracy: 0.52000
Iteration 1220000, batch loss: 0.007210, training accuracy: 0.52000
Iteration 1230000, batch loss: 0.006033, training accuracy: 0.66000
Iteration 1240000, batch loss: 0.007152, training accuracy: 0.44000
Iteration 1250000, batch loss: 0.006137, training accuracy: 0.54000
Iteration 1260000, batch loss: 0.007157, training accuracy: 0.50000
Iteration 1270000, batch loss: 0.006448, training accuracy: 0.52000
Iteration 1280000, batch loss: 0.007090, training accuracy: 0.46000
Iteration 1290000, batch loss: 0.005658, training accuracy: 0.58000
Iteration 1300000, batch loss: 0.007292, training accuracy: 0.58000
Iteration 1310000, batch loss: 0.007057, training accuracy: 0.48000
Iteration 1320000, batch loss: 0.006899, training accuracy: 0.52000
Iteration 1330000, batch loss: 0.006198, training accuracy: 0.56000
Iteration 1340000, batch loss: 0.006871, training accuracy: 0.46000
Iteration 1350000, batch loss: 0.006724, training accuracy: 0.50000
Iteration 1360000, batch loss: 0.006480, training accuracy: 0.50000
Iteration 1370000, batch loss: 0.007141, training accuracy: 0.46000
Iteration 1380000, batch loss: 0.007929, training accuracy: 0.42000
Iteration 1390000, batch loss: 0.006834, training accuracy: 0.50000
Iteration 1400000, batch loss: 0.005786, training accuracy: 0.58000
Iteration 1410000, batch loss: 0.006567, training accuracy: 0.52000
Iteration 1420000, batch loss: 0.006316, training accuracy: 0.56000
Iteration 1430000, batch loss: 0.007422, training accuracy: 0.46000
Iteration 1440000, batch loss: 0.006004, training accuracy: 0.56000
Iteration 1450000, batch loss: 0.007175, training accuracy: 0.46000
Iteration 1460000, batch loss: 0.007782, training accuracy: 0.42000
Iteration 1470000, batch loss: 0.006701, training accuracy: 0.50000
Iteration 1480000, batch loss: 0.006875, training accuracy: 0.50000
Iteration 1490000, batch loss: 0.005742, training accuracy: 0.66000
Iteration 1500000, batch loss: 0.005805, training accuracy: 0.62000
Iteration 1510000, batch loss: 0.006668, training accuracy: 0.48000
Iteration 1520000, batch loss: 0.006347, training accuracy: 0.60000
Iteration 1530000, batch loss: 0.007434, training accuracy: 0.52000
Iteration 1540000, batch loss: 0.005726, training accuracy: 0.58000
Iteration 1550000, batch loss: 0.005737, training accuracy: 0.70000
Iteration 1560000, batch loss: 0.006548, training accuracy: 0.58000
Iteration 1570000, batch loss: 0.006186, training accuracy: 0.58000
Iteration 1580000, batch loss: 0.007047, training accuracy: 0.46000
Iteration 1590000, batch loss: 0.006645, training accuracy: 0.50000
Iteration 1600000, batch loss: 0.006565, training accuracy: 0.52000
Iteration 1610000, batch loss: 0.006548, training accuracy: 0.50000
Iteration 1620000, batch loss: 0.006187, training accuracy: 0.58000
Iteration 1630000, batch loss: 0.006219, training accuracy: 0.56000
Iteration 1640000, batch loss: 0.004125, training accuracy: 0.70000
Iteration 1650000, batch loss: 0.006518, training accuracy: 0.58000
Iteration 1660000, batch loss: 0.006253, training accuracy: 0.58000
Iteration 1670000, batch loss: 0.006567, training accuracy: 0.52000
Iteration 1680000, batch loss: 0.006190, training accuracy: 0.54000
Iteration 1690000, batch loss: 0.006166, training accuracy: 0.56000
Iteration 1700000, batch loss: 0.007062, training accuracy: 0.52000
Iteration 1710000, batch loss: 0.005644, training accuracy: 0.62000
Iteration 1720000, batch loss: 0.005540, training accuracy: 0.70000
Iteration 1730000, batch loss: 0.006545, training accuracy: 0.52000
Iteration 1740000, batch loss: 0.006186, training accuracy: 0.54000
Iteration 1750000, batch loss: 0.006743, training accuracy: 0.52000
Iteration 1760000, batch loss: 0.006357, training accuracy: 0.52000
Iteration 1770000, batch loss: 0.006325, training accuracy: 0.58000
Iteration 1780000, batch loss: 0.005701, training accuracy: 0.54000
Iteration 1790000, batch loss: 0.007482, training accuracy: 0.36000
Iteration 1800000, batch loss: 0.005827, training accuracy: 0.60000
Iteration 1810000, batch loss: 0.006084, training accuracy: 0.58000
Iteration 1820000, batch loss: 0.005192, training accuracy: 0.62000
Iteration 1830000, batch loss: 0.005917, training accuracy: 0.56000
Iteration 1840000, batch loss: 0.005937, training accuracy: 0.62000
Iteration 1850000, batch loss: 0.005859, training accuracy: 0.66000
Iteration 1860000, batch loss: 0.006511, training accuracy: 0.48000
Iteration 1870000, batch loss: 0.006835, training accuracy: 0.46000
Iteration 1880000, batch loss: 0.006248, training accuracy: 0.56000
Iteration 1890000, batch loss: 0.006570, training accuracy: 0.52000
Iteration 1900000, batch loss: 0.007679, training accuracy: 0.38000
Iteration 1910000, batch loss: 0.005331, training accuracy: 0.64000
Iteration 1920000, batch loss: 0.005494, training accuracy: 0.56000
Iteration 1930000, batch loss: 0.005275, training accuracy: 0.64000
Iteration 1940000, batch loss: 0.004943, training accuracy: 0.66000
Iteration 1950000, batch loss: 0.005059, training accuracy: 0.70000
Iteration 1960000, batch loss: 0.006025, training accuracy: 0.60000
Iteration 1970000, batch loss: 0.007004, training accuracy: 0.44000
Iteration 1980000, batch loss: 0.007056, training accuracy: 0.46000
Iteration 1990000, batch loss: 0.005501, training accuracy: 0.58000
Iteration 2000000, batch loss: 0.005497, training accuracy: 0.58000
Iteration 2010000, batch loss: 0.005395, training accuracy: 0.64000
Iteration 2020000, batch loss: 0.005776, training accuracy: 0.54000
Iteration 2030000, batch loss: 0.006656, training accuracy: 0.52000
Iteration 2040000, batch loss: 0.005486, training accuracy: 0.62000
Iteration 2050000, batch loss: 0.006208, training accuracy: 0.56000
Iteration 2060000, batch loss: 0.004686, training accuracy: 0.64000
Iteration 2070000, batch loss: 0.004206, training accuracy: 0.72000
Iteration 2080000, batch loss: 0.004155, training accuracy: 0.74000
Iteration 2090000, batch loss: 0.004781, training accuracy: 0.66000
Iteration 2100000, batch loss: 0.005143, training accuracy: 0.72000
Iteration 2110000, batch loss: 0.004726, training accuracy: 0.66000
Iteration 2120000, batch loss: 0.006733, training accuracy: 0.46000
Iteration 2130000, batch loss: 0.004808, training accuracy: 0.64000
Iteration 2140000, batch loss: 0.005259, training accuracy: 0.62000
Iteration 2150000, batch loss: 0.004761, training accuracy: 0.66000
Iteration 2160000, batch loss: 0.004712, training accuracy: 0.68000
Iteration 2170000, batch loss: 0.004156, training accuracy: 0.76000
Iteration 2180000, batch loss: 0.005027, training accuracy: 0.66000
Iteration 2190000, batch loss: 0.004595, training accuracy: 0.64000
Iteration 2200000, batch loss: 0.005482, training accuracy: 0.64000
Iteration 2210000, batch loss: 0.005621, training accuracy: 0.56000
Iteration 2220000, batch loss: 0.004738, training accuracy: 0.68000
Iteration 2230000, batch loss: 0.004815, training accuracy: 0.70000
Iteration 2240000, batch loss: 0.004667, training accuracy: 0.66000
Iteration 2250000, batch loss: 0.004451, training accuracy: 0.70000
Iteration 2260000, batch loss: 0.003568, training accuracy: 0.72000
Iteration 2270000, batch loss: 0.004044, training accuracy: 0.78000
Iteration 2280000, batch loss: 0.003895, training accuracy: 0.80000
Iteration 2290000, batch loss: 0.003101, training accuracy: 0.86000
Iteration 2300000, batch loss: 0.005553, training accuracy: 0.60000
Iteration 2310000, batch loss: 0.004286, training accuracy: 0.68000
Iteration 2320000, batch loss: 0.005825, training accuracy: 0.58000
Iteration 2330000, batch loss: 0.003917, training accuracy: 0.72000
Iteration 2340000, batch loss: 0.002382, training accuracy: 0.84000
Iteration 2350000, batch loss: 0.005534, training accuracy: 0.56000
Iteration 2360000, batch loss: 0.004697, training accuracy: 0.64000
Iteration 2370000, batch loss: 0.004096, training accuracy: 0.72000
Iteration 2380000, batch loss: 0.004590, training accuracy: 0.64000
Iteration 2390000, batch loss: 0.003473, training accuracy: 0.76000
Iteration 2400000, batch loss: 0.004698, training accuracy: 0.68000
Iteration 2410000, batch loss: 0.004589, training accuracy: 0.70000
Iteration 2420000, batch loss: 0.003804, training accuracy: 0.78000
Iteration 2430000, batch loss: 0.003333, training accuracy: 0.78000
Iteration 2440000, batch loss: 0.002827, training accuracy: 0.84000
Iteration 2450000, batch loss: 0.005235, training accuracy: 0.60000
Iteration 2460000, batch loss: 0.003296, training accuracy: 0.78000
Iteration 2470000, batch loss: 0.003313, training accuracy: 0.80000
Iteration 2480000, batch loss: 0.003724, training accuracy: 0.74000
Iteration 2490000, batch loss: 0.003535, training accuracy: 0.80000
Iteration 2500000, batch loss: 0.004641, training accuracy: 0.64000
Iteration 2510000, batch loss: 0.003985, training accuracy: 0.76000
Iteration 2520000, batch loss: 0.003422, training accuracy: 0.78000
Iteration 2530000, batch loss: 0.004706, training accuracy: 0.66000
Iteration 2540000, batch loss: 0.003821, training accuracy: 0.72000
Iteration 2550000, batch loss: 0.003921, training accuracy: 0.74000
Iteration 2560000, batch loss: 0.003530, training accuracy: 0.78000
Iteration 2570000, batch loss: 0.004089, training accuracy: 0.76000
Iteration 2580000, batch loss: 0.002646, training accuracy: 0.82000
Iteration 2590000, batch loss: 0.004256, training accuracy: 0.70000
Iteration 2600000, batch loss: 0.004265, training accuracy: 0.70000
Iteration 2610000, batch loss: 0.002910, training accuracy: 0.80000
Iteration 2620000, batch loss: 0.005123, training accuracy: 0.60000
Iteration 2630000, batch loss: 0.002963, training accuracy: 0.84000
Iteration 2640000, batch loss: 0.004242, training accuracy: 0.68000
Iteration 2650000, batch loss: 0.004260, training accuracy: 0.68000
Iteration 2660000, batch loss: 0.003785, training accuracy: 0.74000
Iteration 2670000, batch loss: 0.003668, training accuracy: 0.76000
Iteration 2680000, batch loss: 0.003629, training accuracy: 0.76000
Iteration 2690000, batch loss: 0.004283, training accuracy: 0.70000
Iteration 2700000, batch loss: 0.004085, training accuracy: 0.74000
Iteration 2710000, batch loss: 0.003585, training accuracy: 0.74000
Iteration 2720000, batch loss: 0.003974, training accuracy: 0.70000
Iteration 2730000, batch loss: 0.003181, training accuracy: 0.80000
Iteration 2740000, batch loss: 0.004028, training accuracy: 0.72000
Iteration 2750000, batch loss: 0.004444, training accuracy: 0.72000
Iteration 2760000, batch loss: 0.003452, training accuracy: 0.74000
Iteration 2770000, batch loss: 0.004009, training accuracy: 0.78000
Iteration 2780000, batch loss: 0.003804, training accuracy: 0.78000
Iteration 2790000, batch loss: 0.004683, training accuracy: 0.66000
Iteration 2800000, batch loss: 0.003999, training accuracy: 0.72000
Iteration 2810000, batch loss: 0.005038, training accuracy: 0.64000
Iteration 2820000, batch loss: 0.004440, training accuracy: 0.68000
Iteration 2830000, batch loss: 0.003551, training accuracy: 0.76000
Iteration 2840000, batch loss: 0.002723, training accuracy: 0.84000
Iteration 2850000, batch loss: 0.004190, training accuracy: 0.70000
Iteration 2860000, batch loss: 0.004108, training accuracy: 0.66000
Iteration 2870000, batch loss: 0.003278, training accuracy: 0.80000
Iteration 2880000, batch loss: 0.004370, training accuracy: 0.66000
Iteration 2890000, batch loss: 0.003670, training accuracy: 0.76000
Iteration 2900000, batch loss: 0.003597, training accuracy: 0.74000
Iteration 2910000, batch loss: 0.004004, training accuracy: 0.72000
Iteration 2920000, batch loss: 0.002591, training accuracy: 0.82000
Iteration 2930000, batch loss: 0.003721, training accuracy: 0.76000
Iteration 2940000, batch loss: 0.002649, training accuracy: 0.82000
Iteration 2950000, batch loss: 0.004596, training accuracy: 0.70000
Iteration 2960000, batch loss: 0.003287, training accuracy: 0.78000
Iteration 2970000, batch loss: 0.004253, training accuracy: 0.72000
Iteration 2980000, batch loss: 0.004535, training accuracy: 0.72000
Iteration 2990000, batch loss: 0.002726, training accuracy: 0.86000
Iteration 3000000, batch loss: 0.003490, training accuracy: 0.76000
Iteration 3010000, batch loss: 0.003947, training accuracy: 0.74000
Iteration 3020000, batch loss: 0.004773, training accuracy: 0.62000
Iteration 3030000, batch loss: 0.004558, training accuracy: 0.64000
Iteration 3040000, batch loss: 0.003957, training accuracy: 0.70000
Iteration 3050000, batch loss: 0.003210, training accuracy: 0.78000
Iteration 3060000, batch loss: 0.003308, training accuracy: 0.76000
Iteration 3070000, batch loss: 0.003939, training accuracy: 0.78000
Iteration 3080000, batch loss: 0.003297, training accuracy: 0.82000
Iteration 3090000, batch loss: 0.002858, training accuracy: 0.82000
Iteration 3100000, batch loss: 0.003763, training accuracy: 0.74000
Iteration 3110000, batch loss: 0.004031, training accuracy: 0.76000
Iteration 3120000, batch loss: 0.003228, training accuracy: 0.74000
Iteration 3130000, batch loss: 0.003646, training accuracy: 0.80000
Iteration 3140000, batch loss: 0.002316, training accuracy: 0.86000
Iteration 3150000, batch loss: 0.004221, training accuracy: 0.68000
Iteration 3160000, batch loss: 0.003094, training accuracy: 0.76000
Iteration 3170000, batch loss: 0.003257, training accuracy: 0.76000
Iteration 3180000, batch loss: 0.003150, training accuracy: 0.80000
Iteration 3190000, batch loss: 0.003071, training accuracy: 0.84000
Iteration 3200000, batch loss: 0.002680, training accuracy: 0.80000
Iteration 3210000, batch loss: 0.003372, training accuracy: 0.76000
Iteration 3220000, batch loss: 0.003551, training accuracy: 0.76000
Iteration 3230000, batch loss: 0.004290, training accuracy: 0.66000
Iteration 3240000, batch loss: 0.003659, training accuracy: 0.74000
Iteration 3250000, batch loss: 0.003180, training accuracy: 0.78000
Iteration 3260000, batch loss: 0.003589, training accuracy: 0.76000
Iteration 3270000, batch loss: 0.003659, training accuracy: 0.76000
Iteration 3280000, batch loss: 0.002518, training accuracy: 0.84000
Iteration 3290000, batch loss: 0.002677, training accuracy: 0.80000
Iteration 3300000, batch loss: 0.003883, training accuracy: 0.78000
Iteration 3310000, batch loss: 0.004177, training accuracy: 0.66000
Iteration 3320000, batch loss: 0.004031, training accuracy: 0.68000
Iteration 3330000, batch loss: 0.004217, training accuracy: 0.70000
Iteration 3340000, batch loss: 0.003061, training accuracy: 0.82000
Iteration 3350000, batch loss: 0.003394, training accuracy: 0.74000
Iteration 3360000, batch loss: 0.003106, training accuracy: 0.80000
Iteration 3370000, batch loss: 0.004137, training accuracy: 0.76000
Iteration 3380000, batch loss: 0.003284, training accuracy: 0.76000
Iteration 3390000, batch loss: 0.003549, training accuracy: 0.74000
Iteration 3400000, batch loss: 0.003357, training accuracy: 0.76000
Iteration 3410000, batch loss: 0.003753, training accuracy: 0.78000
Iteration 3420000, batch loss: 0.003285, training accuracy: 0.86000
Iteration 3430000, batch loss: 0.002487, training accuracy: 0.86000
Iteration 3440000, batch loss: 0.003375, training accuracy: 0.78000
Iteration 3450000, batch loss: 0.003651, training accuracy: 0.72000
Iteration 3460000, batch loss: 0.003074, training accuracy: 0.78000
Iteration 3470000, batch loss: 0.003557, training accuracy: 0.76000
Iteration 3480000, batch loss: 0.003364, training accuracy: 0.78000
Iteration 3490000, batch loss: 0.003265, training accuracy: 0.74000
Iteration 3500000, batch loss: 0.002757, training accuracy: 0.82000
Iteration 3510000, batch loss: 0.002868, training accuracy: 0.82000
Iteration 3520000, batch loss: 0.002856, training accuracy: 0.80000
Iteration 3530000, batch loss: 0.003097, training accuracy: 0.80000
Iteration 3540000, batch loss: 0.002238, training accuracy: 0.90000
Iteration 3550000, batch loss: 0.002909, training accuracy: 0.80000
Iteration 3560000, batch loss: 0.001935, training accuracy: 0.88000
Iteration 3570000, batch loss: 0.003783, training accuracy: 0.78000
Iteration 3580000, batch loss: 0.002108, training accuracy: 0.86000
Iteration 3590000, batch loss: 0.003339, training accuracy: 0.78000
Iteration 3600000, batch loss: 0.003570, training accuracy: 0.76000
Iteration 3610000, batch loss: 0.002402, training accuracy: 0.84000
Iteration 3620000, batch loss: 0.003739, training accuracy: 0.74000
Iteration 3630000, batch loss: 0.004272, training accuracy: 0.66000
Iteration 3640000, batch loss: 0.003843, training accuracy: 0.70000
Iteration 3650000, batch loss: 0.004192, training accuracy: 0.76000
Iteration 3660000, batch loss: 0.002889, training accuracy: 0.82000
Iteration 3670000, batch loss: 0.003831, training accuracy: 0.72000
Iteration 3680000, batch loss: 0.003829, training accuracy: 0.70000
Iteration 3690000, batch loss: 0.002358, training accuracy: 0.82000
Iteration 3700000, batch loss: 0.002810, training accuracy: 0.80000
Iteration 3710000, batch loss: 0.003450, training accuracy: 0.76000
Iteration 3720000, batch loss: 0.002343, training accuracy: 0.84000
Iteration 3730000, batch loss: 0.003080, training accuracy: 0.84000
Iteration 3740000, batch loss: 0.002914, training accuracy: 0.78000
Iteration 3750000, batch loss: 0.002937, training accuracy: 0.84000
Iteration 3760000, batch loss: 0.003116, training accuracy: 0.82000
Iteration 3770000, batch loss: 0.003012, training accuracy: 0.80000
Iteration 3780000, batch loss: 0.001550, training accuracy: 0.90000
Iteration 3790000, batch loss: 0.003465, training accuracy: 0.76000
Iteration 3800000, batch loss: 0.002692, training accuracy: 0.84000
Iteration 3810000, batch loss: 0.002573, training accuracy: 0.82000
Iteration 3820000, batch loss: 0.002531, training accuracy: 0.82000
Iteration 3830000, batch loss: 0.003269, training accuracy: 0.78000
Iteration 3840000, batch loss: 0.003026, training accuracy: 0.78000
Iteration 3850000, batch loss: 0.003081, training accuracy: 0.80000
Iteration 3860000, batch loss: 0.003588, training accuracy: 0.72000
Iteration 3870000, batch loss: 0.003599, training accuracy: 0.76000
Iteration 3880000, batch loss: 0.001958, training accuracy: 0.90000
Iteration 3890000, batch loss: 0.002311, training accuracy: 0.84000
Iteration 3900000, batch loss: 0.002911, training accuracy: 0.84000
Iteration 3910000, batch loss: 0.003084, training accuracy: 0.80000
Iteration 3920000, batch loss: 0.004098, training accuracy: 0.70000
Iteration 3930000, batch loss: 0.001186, training accuracy: 0.94000
Iteration 3940000, batch loss: 0.003072, training accuracy: 0.80000
Iteration 3950000, batch loss: 0.002850, training accuracy: 0.86000
Iteration 3960000, batch loss: 0.002954, training accuracy: 0.78000
Iteration 3970000, batch loss: 0.003515, training accuracy: 0.78000
Iteration 3980000, batch loss: 0.002477, training accuracy: 0.90000
Iteration 3990000, batch loss: 0.002595, training accuracy: 0.82000
Iteration 4000000, batch loss: 0.003370, training accuracy: 0.76000
Iteration 4010000, batch loss: 0.002319, training accuracy: 0.88000
Iteration 4020000, batch loss: 0.002429, training accuracy: 0.86000
Iteration 4030000, batch loss: 0.002877, training accuracy: 0.80000
Iteration 4040000, batch loss: 0.002416, training accuracy: 0.84000
Iteration 4050000, batch loss: 0.003394, training accuracy: 0.74000
Iteration 4060000, batch loss: 0.002519, training accuracy: 0.86000
Iteration 4070000, batch loss: 0.002386, training accuracy: 0.88000
Iteration 4080000, batch loss: 0.003166, training accuracy: 0.78000
Iteration 4090000, batch loss: 0.003455, training accuracy: 0.74000
Iteration 4100000, batch loss: 0.002830, training accuracy: 0.82000
Iteration 4110000, batch loss: 0.002315, training accuracy: 0.88000
Iteration 4120000, batch loss: 0.002565, training accuracy: 0.82000
Iteration 4130000, batch loss: 0.002218, training accuracy: 0.90000
Iteration 4140000, batch loss: 0.002409, training accuracy: 0.84000
Iteration 4150000, batch loss: 0.002430, training accuracy: 0.86000
Iteration 4160000, batch loss: 0.003071, training accuracy: 0.80000
Iteration 4170000, batch loss: 0.003195, training accuracy: 0.80000
Iteration 4180000, batch loss: 0.001862, training accuracy: 0.88000
Iteration 4190000, batch loss: 0.003183, training accuracy: 0.80000
Iteration 4200000, batch loss: 0.001970, training accuracy: 0.90000
Iteration 4210000, batch loss: 0.002032, training accuracy: 0.88000
Iteration 4220000, batch loss: 0.002771, training accuracy: 0.84000
Iteration 4230000, batch loss: 0.002412, training accuracy: 0.86000
Iteration 4240000, batch loss: 0.003027, training accuracy: 0.80000
Iteration 4250000, batch loss: 0.002397, training accuracy: 0.90000
Iteration 4260000, batch loss: 0.002503, training accuracy: 0.84000
Iteration 4270000, batch loss: 0.002733, training accuracy: 0.80000
Iteration 4280000, batch loss: 0.003848, training accuracy: 0.70000
Iteration 4290000, batch loss: 0.003465, training accuracy: 0.74000
Iteration 4300000, batch loss: 0.003056, training accuracy: 0.80000
Iteration 4310000, batch loss: 0.002686, training accuracy: 0.78000
Iteration 4320000, batch loss: 0.002202, training accuracy: 0.86000
Iteration 4330000, batch loss: 0.003060, training accuracy: 0.80000
Iteration 4340000, batch loss: 0.002247, training accuracy: 0.88000
Iteration 4350000, batch loss: 0.002774, training accuracy: 0.84000
Iteration 4360000, batch loss: 0.003856, training accuracy: 0.72000
Iteration 4370000, batch loss: 0.002496, training accuracy: 0.86000
Iteration 4380000, batch loss: 0.003127, training accuracy: 0.82000
Iteration 4390000, batch loss: 0.004315, training accuracy: 0.66000
Iteration 4400000, batch loss: 0.001953, training accuracy: 0.92000
Iteration 4410000, batch loss: 0.001875, training accuracy: 0.90000
Iteration 4420000, batch loss: 0.003237, training accuracy: 0.76000
Iteration 4430000, batch loss: 0.001924, training accuracy: 0.88000
Iteration 4440000, batch loss: 0.002799, training accuracy: 0.82000
Iteration 4450000, batch loss: 0.003214, training accuracy: 0.80000
Iteration 4460000, batch loss: 0.003493, training accuracy: 0.78000
Iteration 4470000, batch loss: 0.003426, training accuracy: 0.80000
Iteration 4480000, batch loss: 0.002941, training accuracy: 0.80000
Iteration 4490000, batch loss: 0.003396, training accuracy: 0.72000
Iteration 4500000, batch loss: 0.002103, training accuracy: 0.90000
Iteration 4510000, batch loss: 0.002730, training accuracy: 0.78000
Iteration 4520000, batch loss: 0.004364, training accuracy: 0.64000
Iteration 4530000, batch loss: 0.003214, training accuracy: 0.80000
Iteration 4540000, batch loss: 0.003871, training accuracy: 0.74000
Iteration 4550000, batch loss: 0.002391, training accuracy: 0.90000
Iteration 4560000, batch loss: 0.002790, training accuracy: 0.78000
Iteration 4570000, batch loss: 0.002293, training accuracy: 0.84000
Iteration 4580000, batch loss: 0.002629, training accuracy: 0.82000
Iteration 4590000, batch loss: 0.002173, training accuracy: 0.86000
Iteration 4600000, batch loss: 0.002580, training accuracy: 0.84000
Iteration 4610000, batch loss: 0.003432, training accuracy: 0.78000
Iteration 4620000, batch loss: 0.002766, training accuracy: 0.84000
Iteration 4630000, batch loss: 0.002728, training accuracy: 0.82000
Iteration 4640000, batch loss: 0.003014, training accuracy: 0.78000
Iteration 4650000, batch loss: 0.002328, training accuracy: 0.84000
Iteration 4660000, batch loss: 0.001939, training accuracy: 0.92000
Iteration 4670000, batch loss: 0.002285, training accuracy: 0.84000
Iteration 4680000, batch loss: 0.002551, training accuracy: 0.84000
Iteration 4690000, batch loss: 0.003008, training accuracy: 0.82000
Iteration 4700000, batch loss: 0.003042, training accuracy: 0.82000
Iteration 4710000, batch loss: 0.001644, training accuracy: 0.90000
Iteration 4720000, batch loss: 0.002301, training accuracy: 0.88000
Iteration 4730000, batch loss: 0.001622, training accuracy: 0.94000
Iteration 4740000, batch loss: 0.002568, training accuracy: 0.82000
Iteration 4750000, batch loss: 0.002396, training accuracy: 0.84000
Iteration 4760000, batch loss: 0.001885, training accuracy: 0.88000
Iteration 4770000, batch loss: 0.001759, training accuracy: 0.90000
Iteration 4780000, batch loss: 0.001926, training accuracy: 0.90000
Iteration 4790000, batch loss: 0.002649, training accuracy: 0.86000
Iteration 4800000, batch loss: 0.002254, training accuracy: 0.82000
Iteration 4810000, batch loss: 0.002500, training accuracy: 0.86000
Iteration 4820000, batch loss: 0.002168, training accuracy: 0.88000
Iteration 4830000, batch loss: 0.001070, training accuracy: 0.94000
Iteration 4840000, batch loss: 0.003278, training accuracy: 0.82000
Iteration 4850000, batch loss: 0.002759, training accuracy: 0.82000
Iteration 4860000, batch loss: 0.001846, training accuracy: 0.90000
Iteration 4870000, batch loss: 0.002124, training accuracy: 0.88000
Iteration 4880000, batch loss: 0.002332, training accuracy: 0.82000
Iteration 4890000, batch loss: 0.002274, training accuracy: 0.84000
Iteration 4900000, batch loss: 0.002590, training accuracy: 0.82000
Iteration 4910000, batch loss: 0.001555, training accuracy: 0.92000
Iteration 4920000, batch loss: 0.001756, training accuracy: 0.92000
Iteration 4930000, batch loss: 0.000945, training accuracy: 1.00000
Iteration 4940000, batch loss: 0.002313, training accuracy: 0.86000
Iteration 4950000, batch loss: 0.002339, training accuracy: 0.86000
Iteration 4960000, batch loss: 0.001549, training accuracy: 0.94000
Iteration 4970000, batch loss: 0.002304, training accuracy: 0.84000
Iteration 4980000, batch loss: 0.001432, training accuracy: 0.96000
Iteration 4990000, batch loss: 0.003034, training accuracy: 0.76000
Iteration 5000000, batch loss: 0.002337, training accuracy: 0.84000
Iteration 5010000, batch loss: 0.002307, training accuracy: 0.82000
Iteration 5020000, batch loss: 0.002908, training accuracy: 0.80000
Iteration 5030000, batch loss: 0.002610, training accuracy: 0.82000
Iteration 5040000, batch loss: 0.002258, training accuracy: 0.86000
Iteration 5050000, batch loss: 0.002254, training accuracy: 0.88000
Iteration 5060000, batch loss: 0.002246, training accuracy: 0.88000
Iteration 5070000, batch loss: 0.001608, training accuracy: 0.94000
Iteration 5080000, batch loss: 0.002281, training accuracy: 0.82000
Iteration 5090000, batch loss: 0.002510, training accuracy: 0.84000
Iteration 5100000, batch loss: 0.001782, training accuracy: 0.88000
Iteration 5110000, batch loss: 0.002943, training accuracy: 0.82000
Iteration 5120000, batch loss: 0.001485, training accuracy: 0.90000
Iteration 5130000, batch loss: 0.003244, training accuracy: 0.82000
Iteration 5140000, batch loss: 0.002296, training accuracy: 0.86000
Iteration 5150000, batch loss: 0.002079, training accuracy: 0.84000
Iteration 5160000, batch loss: 0.001836, training accuracy: 0.90000
Iteration 5170000, batch loss: 0.002615, training accuracy: 0.84000
Iteration 5180000, batch loss: 0.002893, training accuracy: 0.84000
Iteration 5190000, batch loss: 0.002187, training accuracy: 0.88000
Iteration 5200000, batch loss: 0.002390, training accuracy: 0.84000
Iteration 5210000, batch loss: 0.002533, training accuracy: 0.82000
Iteration 5220000, batch loss: 0.002434, training accuracy: 0.82000
Iteration 5230000, batch loss: 0.002107, training accuracy: 0.94000
Iteration 5240000, batch loss: 0.002961, training accuracy: 0.84000
Iteration 5250000, batch loss: 0.002309, training accuracy: 0.84000
Iteration 5260000, batch loss: 0.002723, training accuracy: 0.80000
Iteration 5270000, batch loss: 0.002285, training accuracy: 0.88000
Iteration 5280000, batch loss: 0.002694, training accuracy: 0.84000
Iteration 5290000, batch loss: 0.002400, training accuracy: 0.84000
Iteration 5300000, batch loss: 0.002931, training accuracy: 0.80000
